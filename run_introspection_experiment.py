"""
Run introspection experiment: Test whether models use internal uncertainty signals for meta-cognition.

This script:
1. Loads MC questions and runs them through the model in two modes:
   - Direct: Ask the MC question directly, compute uncertainty metrics over A/B/C/D
   - Meta: Ask "How confident are you that you know the answer to [Q]?"
2. Extracts activations from both prompt types
3. Computes multiple uncertainty metrics (all saved, one probed per run):
   - Prob-based (nonlinear): entropy, top_prob, margin
   - Logit-based (linear): logit_gap, top_logit
4. Trains a linear probe on direct activations → selected metric
5. Tests whether that probe transfers to meta activations → direct metric
   (If it does, the model may be "introspecting" on an internal uncertainty signal)

Key insight: If the model is truly introspecting when answering meta-questions,
it should internally access the same representations it would use for the direct case.
A probe trained on direct data should therefore transfer to meta data.

Usage:
    python run_introspection_experiment.py --metric logit_gap   # Probe logit_gap (default)
    python run_introspection_experiment.py --metric entropy     # Probe entropy
"""

import argparse
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Tuple, Optional
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy.stats import pearsonr, spearmanr
import os
from dotenv import load_dotenv
import random
import pickle

from core.model_utils import load_model_and_tokenizer, DEVICE, HF_TOKEN
from tasks import (
    # Direct MC task
    MC_SETUP_PROMPT,
    format_direct_prompt,
    # Confidence task
    STATED_CONFIDENCE_SETUP,
    STATED_CONFIDENCE_OPTIONS,
    STATED_CONFIDENCE_MIDPOINTS,
    STATED_CONFIDENCE_QUESTION,
    format_stated_confidence_prompt,
    get_stated_confidence_signal,
    # Other-confidence task (control: estimate human difficulty)
    OTHER_CONFIDENCE_SETUP,
    OTHER_CONFIDENCE_QUESTION,
    format_other_confidence_prompt,
    get_other_confidence_signal,
    # Delegate task
    ANSWER_OR_DELEGATE_SETUP,
    ANSWER_OR_DELEGATE_SYSPROMPT,
    ANSWER_OR_DELEGATE_OPTIONS,
    format_answer_or_delegate_prompt,
    get_delegate_mapping,
    # Unified conversion
    response_to_confidence,
)

load_dotenv()

# Configuration
BASE_MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"
MODEL_NAME = BASE_MODEL_NAME  # Set to adapter path if using fine-tuned model

# Lists of datasets and meta_tasks to process (will iterate through all combinations)
# Set to a single-item list for single runs, or multiple items to batch process
DATASETS = ["SimpleMC", "TriviaMC"]  # Options: "SimpleMC", "TriviaMC", "GPQA", etc.
META_TASKS = ["confidence", "delegate"]  # Options: "confidence", "delegate"

# Legacy single-value variables (used by functions that reference them)
DATASET_NAME = DATASETS[0]  # Will be updated during iteration
META_TASK = META_TASKS[0]  # Will be updated during iteration

# Number of questions per dataset (can be overridden per-dataset below)
NUM_QUESTIONS_DEFAULT = 500
NUM_QUESTIONS_BY_DATASET = {
    "GPQA": 447,  # GPQA has fewer questions
}
NUM_QUESTIONS = NUM_QUESTIONS_BY_DATASET.get(DATASET_NAME, NUM_QUESTIONS_DEFAULT)

# DEVICE imported from core.model_utils
SEED = 42

# Quantization settings (auto-detect for large models, can override via CLI)
LOAD_IN_4BIT = True if "70B" in BASE_MODEL_NAME else False
LOAD_IN_8BIT = False

# Output directory
OUTPUTS_DIR = Path("outputs")
OUTPUTS_DIR.mkdir(exist_ok=True)

# Probe training config
TRAIN_SPLIT = 0.8
PROBE_ALPHA = 1000.0
USE_PCA = True
PCA_COMPONENTS = 100

# Option to load pre-trained probe from run_mc_experiment.py
LOAD_PRETRAINED_PROBE = False
PRETRAINED_PROBE_PATH = "mc_probe_trained.pkl"  # If saved from run_mc_experiment

# Available uncertainty metrics (same as mc_entropy_probe.py):
# Prob-based (nonlinear targets - may be harder for linear probes):
#   entropy   - Shannon entropy -sum(p * log(p))
#   top_prob  - P(argmax) - probability of most likely answer
#   margin    - P(top) - P(second) - prob gap between top two
# Logit-based (linear targets - better aligned with linear probes):
#   logit_gap - z(top) - z(second) - logit gap between top two
#   top_logit - z(top) - mean(z) - centered top logit
AVAILABLE_METRICS = ["entropy", "top_prob", "margin", "logit_gap", "top_logit"]
# Metrics where high value = uncertain (vs high value = confident)
UNCERTAINTY_METRICS = {"entropy"}  # Only entropy; others are confidence metrics
METRIC = "entropy"  # Which metric to probe (set via --metric flag)

np.random.seed(SEED)
torch.manual_seed(SEED)
random.seed(SEED)


def get_model_short_name(model_name: str) -> str:
    """Extract a short, filesystem-safe name from a model path."""
    if "/" in model_name:
        parts = model_name.split("/")
        return parts[-1]
    return model_name


def get_output_prefix(metric: str = None) -> str:
    """Generate output filename prefix based on config.

    Args:
        metric: If provided, include metric in prefix (for metric-specific outputs).
                If None, return base prefix (for shared outputs like activations).
    """
    model_short = get_model_short_name(BASE_MODEL_NAME)
    # Include meta task type in output prefix for clarity
    task_suffix = f"_{META_TASK}" if META_TASK != "confidence" else ""
    metric_suffix = f"_{metric}" if metric else ""
    if MODEL_NAME != BASE_MODEL_NAME:
        adapter_short = get_model_short_name(MODEL_NAME)
        return str(OUTPUTS_DIR / f"{model_short}_adapter-{adapter_short}_{DATASET_NAME}_introspection{task_suffix}{metric_suffix}")
    return str(OUTPUTS_DIR / f"{model_short}_{DATASET_NAME}_introspection{task_suffix}{metric_suffix}")


def get_directions_prefix(metric: str = None) -> str:
    """Generate output filename prefix for direction files (task-independent).

    Direction files are task-independent because they're trained on direct task
    activations predicting metrics like entropy/top_logit - the meta task
    (confidence vs delegate) doesn't affect the direction computation.

    Args:
        metric: If provided, include metric in prefix.
    """
    model_short = get_model_short_name(BASE_MODEL_NAME)
    # NO task suffix - directions are task-independent
    metric_suffix = f"_{metric}" if metric else ""
    if MODEL_NAME != BASE_MODEL_NAME:
        adapter_short = get_model_short_name(MODEL_NAME)
        return str(OUTPUTS_DIR / f"{model_short}_adapter-{adapter_short}_{DATASET_NAME}_introspection{metric_suffix}")
    return str(OUTPUTS_DIR / f"{model_short}_{DATASET_NAME}_introspection{metric_suffix}")


# ============================================================================
# BACKWARD COMPATIBILITY ALIASES (now imported from tasks.py)
# ============================================================================

# Confidence task aliases
META_SETUP_PROMPT = STATED_CONFIDENCE_SETUP
META_QUESTION_PROMPT = STATED_CONFIDENCE_QUESTION
META_OPTION_DICT = STATED_CONFIDENCE_OPTIONS
META_RANGE_MIDPOINTS = STATED_CONFIDENCE_MIDPOINTS

# Delegate task aliases
DELEGATE_SETUP_PROMPT = ANSWER_OR_DELEGATE_SETUP
DELEGATE_SYSPROMPT = ANSWER_OR_DELEGATE_SYSPROMPT
DELEGATE_OPTIONS = ANSWER_OR_DELEGATE_OPTIONS


# Import utility functions from core (instead of duplicating)
from core.model_utils import is_base_model, has_chat_template


# ============================================================================
# QUESTION LOADING AND FORMATTING
# ============================================================================

def load_questions(dataset_name: str, num_questions: int = None) -> List[Dict]:
    """Load MC questions using load_and_format_dataset."""
    from load_and_format_datasets import load_and_format_dataset

    questions = load_and_format_dataset(dataset_name, num_questions_needed=num_questions)

    if questions is None:
        raise ValueError(f"Failed to load dataset: {dataset_name}")

    return questions


# Use formatting functions from tasks.py (imported at top)
# format_direct_prompt - imported directly
# Local wrappers for meta tasks to maintain backward compatibility


def format_meta_prompt(question: Dict, tokenizer, use_chat_template: bool = True) -> Tuple[str, List[str]]:
    """Format a meta/confidence question using centralized tasks.py logic."""
    return format_stated_confidence_prompt(question, tokenizer, use_chat_template)


def format_delegate_prompt(
    question: Dict,
    tokenizer,
    use_chat_template: bool = True,
    trial_index: int = 0
) -> Tuple[str, List[str], Dict[str, str]]:
    """Format a delegate question using centralized tasks.py logic."""
    return format_answer_or_delegate_prompt(
        question, tokenizer, trial_index=trial_index,
        alternate_mapping=True, use_chat_template=use_chat_template
    )


def get_meta_prompt_formatter():
    """Return the appropriate prompt formatter based on META_TASK setting."""
    if META_TASK == "delegate":
        return format_delegate_prompt
    else:
        return format_meta_prompt


def get_meta_options():
    """Return the meta options based on META_TASK setting."""
    if META_TASK == "delegate":
        return DELEGATE_OPTIONS
    else:
        return list(META_OPTION_DICT.keys())


def local_response_to_confidence(
    response: str,
    probs: np.ndarray = None,
    mapping: Dict[str, str] = None
) -> float:
    """
    Convert a meta response to a confidence value.

    Wrapper around tasks.response_to_confidence that passes the correct task_type.
    """
    task_type = "delegate" if META_TASK == "delegate" else "confidence"
    return response_to_confidence(response, probs, mapping, task_type)


# ============================================================================
# ENTROPY AND PROBABILITY COMPUTATION
# ============================================================================

def compute_entropy_from_probs(probs: np.ndarray) -> float:
    """Compute entropy from a probability distribution."""
    probs = probs / probs.sum()
    probs = probs[probs > 0]
    entropy = -(probs * np.log(probs)).sum()
    return float(entropy)


def compute_uncertainty_metrics(probs: np.ndarray, logits: np.ndarray = None) -> Dict[str, float]:
    """
    Compute multiple uncertainty metrics from probability and logit distributions.

    Args:
        probs: Probability distribution over answer options (sums to 1)
        logits: Raw logits for answer options (before softmax). If None, logit-based
                metrics will be computed from log(probs) as an approximation.

    Returns:
        Dict with keys: entropy, top_prob, margin, logit_gap, top_logit
    """
    # === Prob-based metrics (nonlinear) ===

    # Entropy: -sum(p * log(p))
    entropy = compute_entropy_from_probs(probs)

    # Top probability: P(argmax)
    top_prob = float(np.max(probs))

    # Margin: P(top) - P(second)
    sorted_probs = np.sort(probs)[::-1]  # Descending
    margin = float(sorted_probs[0] - sorted_probs[1]) if len(sorted_probs) > 1 else float(sorted_probs[0])

    # === Logit-based metrics (linear - better for linear probes) ===

    # If logits not provided, approximate from log-probs
    # (This loses the constant offset but preserves gaps)
    if logits is None:
        logits = np.log(probs + 1e-10)

    # Sort logits descending
    sorted_logits = np.sort(logits)[::-1]

    # Logit gap: z(top) - z(second)
    # This is the cleanest linear target - invariant to temperature/scale shifts
    logit_gap = float(sorted_logits[0] - sorted_logits[1]) if len(sorted_logits) > 1 else float(sorted_logits[0])

    # Top logit (centered): z(top) - mean(z)
    # Subtracting mean makes it invariant to adding a constant to all logits
    top_logit = float(sorted_logits[0] - np.mean(logits))

    return {
        "entropy": entropy,
        "top_prob": top_prob,
        "margin": margin,
        "logit_gap": logit_gap,
        "top_logit": top_logit,
    }


# ============================================================================
# KV CACHE UTILITIES FOR PREFIX SHARING
# ============================================================================

try:
    from transformers.cache_utils import DynamicCache
except ImportError:
    DynamicCache = None


def extract_cache_tensors(past_key_values):
    """Extract immutable tensors from cache object."""
    keys, values = [], []
    try:
        num_layers = len(past_key_values)
    except TypeError:
        if hasattr(past_key_values, "to_legacy_cache"):
            return extract_cache_tensors(past_key_values.to_legacy_cache())
        raise ValueError(f"Cannot determine length of cache: {type(past_key_values)}")
    for i in range(num_layers):
        k, v = past_key_values[i]
        keys.append(k)
        values.append(v)
    return keys, values


def create_fresh_cache(key_tensors, value_tensors, expand_size=1):
    """Reconstruct a fresh cache object from tensors."""
    if DynamicCache is not None:
        cache = DynamicCache()
        for i, (k, v) in enumerate(zip(key_tensors, value_tensors)):
            if expand_size > 1:
                k = k.repeat_interleave(expand_size, dim=0)
                v = v.repeat_interleave(expand_size, dim=0)
            cache.update(k, v, i)
        return cache
    else:
        layers = []
        for k, v in zip(key_tensors, value_tensors):
            if expand_size > 1:
                k = k.repeat_interleave(expand_size, dim=0)
                v = v.repeat_interleave(expand_size, dim=0)
            layers.append((k, v))
        return tuple(layers)


# ============================================================================
# BATCHED ACTIVATION + LOGIT EXTRACTION
# ============================================================================

class BatchedExtractor:
    """Extract activations and logits in a single batched forward pass.

    Optimized to:
    1. Store only last-token activations in hooks (reduces memory by seq_len×)
    2. Do single CPU transfer per batch (reduces GPU syncs from layers×batch to 1)
    """

    def __init__(self, model, num_layers: int):
        self.model = model
        self.num_layers = num_layers
        self.activations = {}
        self.hooks = []

    def _make_hook(self, layer_idx: int):
        def hook(module, input, output):
            if isinstance(output, tuple):
                hidden_states = output[0]
            else:
                hidden_states = output
            # With left-padding, last position (-1) is always the final real token
            # Store only last-token activations: (batch_size, hidden_dim)
            self.activations[layer_idx] = hidden_states[:, -1, :].detach()
        return hook

    def register_hooks(self):
        if hasattr(self.model, 'get_base_model'):
            base = self.model.get_base_model()
            layers = base.model.layers
        else:
            layers = self.model.model.layers

        for i, layer in enumerate(layers):
            hook = self._make_hook(i)
            handle = layer.register_forward_hook(hook)
            self.hooks.append(handle)

    def remove_hooks(self):
        for handle in self.hooks:
            handle.remove()
        self.hooks = []

    def extract_batch(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        option_token_ids: List[int]
    ) -> Tuple[List[Dict[int, np.ndarray]], List[np.ndarray], List[np.ndarray], List[Dict[str, float]]]:
        """
        Extract activations AND compute option probabilities/metrics in one forward pass.

        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
            option_token_ids: List of token IDs for the options

        Returns:
            layer_activations: List of {layer_idx: activation} dicts, one per batch item
            option_probs: List of probability arrays, one per batch item
            option_logits: List of logit arrays, one per batch item
            all_metrics: List of metric dicts, one per batch item
        """
        self.activations = {}
        batch_size = input_ids.shape[0]

        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False)

        # Single CPU transfer: stack all layers and transfer at once
        # self.activations[layer_idx] is already (batch_size, hidden_dim) from optimized hook
        stacked = torch.stack([self.activations[i] for i in range(self.num_layers)], dim=0)
        # stacked shape: (num_layers, batch_size, hidden_dim)
        stacked_cpu = stacked.cpu().numpy()

        # Distribute to per-batch-item dicts
        all_layer_activations = []
        for batch_idx in range(batch_size):
            item_activations = {
                layer_idx: stacked_cpu[layer_idx, batch_idx]
                for layer_idx in range(self.num_layers)
            }
            all_layer_activations.append(item_activations)

        # Extract logits and compute probabilities/metrics for each batch item
        all_probs = []
        all_logits = []
        all_metrics = []
        for batch_idx in range(batch_size):
            final_logits = outputs.logits[batch_idx, -1, :]
            option_logits = final_logits[option_token_ids]
            option_logits_np = option_logits.cpu().numpy()
            probs = torch.softmax(option_logits, dim=-1).cpu().numpy()
            metrics = compute_uncertainty_metrics(probs, option_logits_np)
            all_probs.append(probs)
            all_logits.append(option_logits_np)
            all_metrics.append(metrics)

        return all_layer_activations, all_probs, all_logits, all_metrics

    def compute_prefix_cache(self, input_ids: torch.Tensor):
        """Run shared prefix once to get cache snapshot (no hooks)."""
        with torch.inference_mode():
            outputs = self.model(input_ids=input_ids, use_cache=True)
        return extract_cache_tensors(outputs.past_key_values)

    def extract_batch_with_cache(
        self,
        suffix_ids: torch.Tensor,
        prefix_cache_data: Tuple,
        option_token_ids: List[int],
        pad_token_id: int = 0
    ) -> Tuple[List[Dict], List[np.ndarray], List[np.ndarray], List[Dict]]:
        """
        Extract activations AND compute option probabilities/metrics using KV cache prefix.

        Args:
            suffix_ids: (batch_size, suffix_len) - left-padded suffix token IDs
            prefix_cache_data: (keys, values) tuple from compute_prefix_cache
            option_token_ids: List of token IDs for the options
            pad_token_id: Token ID used for padding (default 0)

        Returns:
            Same as extract_batch
        """
        self.activations = {}
        batch_size = suffix_ids.shape[0]

        keys, values = prefix_cache_data
        prefix_len = keys[0].shape[2]
        suffix_len = suffix_ids.shape[1]

        # Build attention mask for prefix + suffix
        mask = torch.ones((batch_size, prefix_len + suffix_len), dtype=torch.long, device=suffix_ids.device)
        # Handle left-padding in suffix
        mask[:, prefix_len:] = (suffix_ids != pad_token_id).long()

        with torch.no_grad():
            outputs = self.model(
                input_ids=suffix_ids,
                attention_mask=mask,
                past_key_values=create_fresh_cache(keys, values, expand_size=batch_size),
                use_cache=False
            )

        # Single CPU transfer: stack all layers and transfer at once
        stacked = torch.stack([self.activations[i] for i in range(self.num_layers)], dim=0)
        stacked_cpu = stacked.cpu().numpy()

        # Distribute to per-batch-item dicts
        all_layer_activations = []
        for batch_idx in range(batch_size):
            item_activations = {
                layer_idx: stacked_cpu[layer_idx, batch_idx]
                for layer_idx in range(self.num_layers)
            }
            all_layer_activations.append(item_activations)

        # Extract logits and compute probabilities/metrics for each batch item
        all_probs = []
        all_logits = []
        all_metrics = []
        for batch_idx in range(batch_size):
            final_logits = outputs.logits[batch_idx, -1, :]
            option_logits = final_logits[option_token_ids]
            option_logits_np = option_logits.cpu().numpy()
            probs = torch.softmax(option_logits, dim=-1).cpu().numpy()
            metrics = compute_uncertainty_metrics(probs, option_logits_np)
            all_probs.append(probs)
            all_logits.append(option_logits_np)
            all_metrics.append(metrics)

        return all_layer_activations, all_probs, all_logits, all_metrics


# ============================================================================
# MAIN DATA COLLECTION
# ============================================================================

# Batch size for processing (adjust based on GPU memory)
BATCH_SIZE = 4  # Conservative default; increase if you have more VRAM


def find_common_prefix_length(input_ids_list: List[List[int]]) -> int:
    """Find the length of the common token prefix across all sequences."""
    if not input_ids_list:
        return 0
    ref_ids = input_ids_list[0]
    min_len = min(len(ids) for ids in input_ids_list)
    common_len = 0
    for i in range(min_len):
        if all(ids[i] == ref_ids[i] for ids in input_ids_list):
            common_len += 1
        else:
            break
    return common_len


def process_prompts_with_prefix_cache(
    prompts: List[str],
    options_list: List[List[str]],
    tokenizer,
    extractor: BatchedExtractor,
    batch_size: int,
    desc: str,
    collect_activations: bool = True
) -> Tuple[List[Dict], List[np.ndarray], List[np.ndarray], List[Dict], List[str]]:
    """
    Process prompts efficiently with prefix caching.

    When prompts share a common prefix (e.g., same system message and instructions),
    computes the KV cache for the prefix once and reuses it for all suffixes.

    Args:
        prompts: List of full prompt strings
        options_list: List of option lists (one per prompt)
        tokenizer: The tokenizer
        extractor: BatchedExtractor instance (with hooks registered)
        batch_size: Batch size for processing
        desc: Progress bar description
        collect_activations: Whether to collect layer activations

    Returns:
        (layer_activations, probs, logits, metrics, responses)
    """
    # 1. Tokenize all prompts
    encodings = tokenizer(prompts, add_special_tokens=False)
    input_ids_list = encodings["input_ids"]

    # 2. Find common prefix
    common_len = find_common_prefix_length(input_ids_list)

    # 3. Compute prefix cache if prefix is substantial (>20 tokens)
    MIN_PREFIX_FOR_CACHE = 20
    if common_len > MIN_PREFIX_FOR_CACHE:
        print(f"  Found common prefix ({common_len} tokens). Computing prefix cache...")
        prefix_ids = torch.tensor([input_ids_list[0][:common_len]], device=DEVICE)
        prefix_cache = extractor.compute_prefix_cache(prefix_ids)
        suffixes = [ids[common_len:] for ids in input_ids_list]
        use_cache = True
    else:
        if common_len > 0:
            print(f"  Common prefix too short ({common_len} tokens). Using standard processing.")
        suffixes = input_ids_list
        prefix_cache = None
        use_cache = False

    # Get pad token id from tokenizer
    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0

    # 4. Process in batches
    results_acts = []
    results_probs = []
    results_logits = []
    results_metrics = []
    results_responses = []

    for b in tqdm(range(0, len(prompts), batch_size), desc=desc):
        batch_suffixes = suffixes[b:b+batch_size]
        batch_opts = options_list[b:b+batch_size]
        actual_batch_size = len(batch_suffixes)

        # Left-pad suffixes to same length
        max_len = max(len(s) for s in batch_suffixes)
        padded = torch.full((actual_batch_size, max_len), pad_token_id, dtype=torch.long, device=DEVICE)
        for i, s in enumerate(batch_suffixes):
            padded[i, max_len-len(s):] = torch.tensor(s, dtype=torch.long, device=DEVICE)

        # Get option token IDs (assume all items in batch have same options)
        opt_ids = [tokenizer.encode(o, add_special_tokens=False)[0] for o in batch_opts[0]]

        if use_cache:
            acts, probs, logits, metrics = extractor.extract_batch_with_cache(
                padded, prefix_cache, opt_ids, pad_token_id=pad_token_id
            )
        else:
            # Build full inputs with attention mask
            mask = (padded != pad_token_id).long()
            acts, probs, logits, metrics = extractor.extract_batch(padded, mask, opt_ids)

        if collect_activations:
            results_acts.extend(acts)
        results_probs.extend(probs)
        results_logits.extend(logits)
        results_metrics.extend(metrics)

        # Determine responses based on argmax
        for i, p in enumerate(probs):
            results_responses.append(batch_opts[i][np.argmax(p)])

    return results_acts, results_probs, results_logits, results_metrics, results_responses


def collect_paired_data(
    questions: List[Dict],
    model,
    tokenizer,
    num_layers: int,
    use_chat_template: bool = True,
    batch_size: int = BATCH_SIZE
) -> Dict:
    """
    Collect activations and uncertainty metrics for both direct and meta prompts.

    Uses batched processing with combined activation+logit extraction
    and KV cache prefix sharing for meta prompts (~2-3x overall speedup).

    Returns dict with:
        - direct_activations: {layer_idx: np.array of shape (n_questions, hidden_dim)}
        - meta_activations: {layer_idx: np.array of shape (n_questions, hidden_dim)}
        - direct_metrics: {metric_name: np.array of shape (n_questions,)}
        - direct_probs: list of prob arrays
        - direct_logits: list of logit arrays
        - meta_entropies: np.array (entropy over confidence options)
        - meta_probs: list of prob arrays over S-Z (or [P("1"), P("2")] for delegate)
        - meta_responses: list of predicted confidence letters (or "1"/"2" for delegate)
        - meta_mappings: list of mappings for delegate task (None for confidence)
        - questions: the question data
    """
    print(f"Collecting paired data for {len(questions)} questions (batch_size={batch_size})...")

    extractor = BatchedExtractor(model, num_layers)
    extractor.register_hooks()

    # Storage
    direct_layer_acts = {i: [] for i in range(num_layers)}
    direct_metrics_lists = {metric: [] for metric in AVAILABLE_METRICS}
    direct_probs_list = []
    direct_logits_list = []

    model.eval()

    # Pre-compute option token IDs for meta task
    meta_options = get_meta_options()

    try:
        # ============ PHASE 1: DIRECT PROMPTS ============
        print("\nProcessing direct prompts...")
        num_batches = (len(questions) + batch_size - 1) // batch_size

        for batch_idx in tqdm(range(num_batches), desc="Direct prompts"):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(questions))
            batch_questions = questions[start_idx:end_idx]

            direct_prompts = []
            direct_options_list = []
            for q in batch_questions:
                prompt, options = format_direct_prompt(q, tokenizer, use_chat_template)
                direct_prompts.append(prompt)
                direct_options_list.append(options)

            # Check if all questions have same options (most MC questions do)
            first_options = direct_options_list[0]
            all_same_options = all(opts == first_options for opts in direct_options_list)

            if all_same_options:
                # Batch process
                direct_option_token_ids = [
                    tokenizer.encode(opt, add_special_tokens=False)[0] for opt in first_options
                ]

                inputs = tokenizer(
                    direct_prompts,
                    return_tensors="pt",
                    padding=True,
                ).to(DEVICE)

                batch_acts, batch_probs, batch_logits, batch_metrics = extractor.extract_batch(
                    inputs["input_ids"],
                    inputs["attention_mask"],
                    direct_option_token_ids
                )

                for acts, probs, logits, metrics in zip(batch_acts, batch_probs, batch_logits, batch_metrics):
                    for layer_idx, act in acts.items():
                        direct_layer_acts[layer_idx].append(act)
                    direct_probs_list.append(probs.tolist())
                    direct_logits_list.append(logits.tolist())
                    for metric_name, metric_val in metrics.items():
                        direct_metrics_lists[metric_name].append(metric_val)

                del inputs
            else:
                # Fall back to per-item processing
                for prompt, options in zip(direct_prompts, direct_options_list):
                    option_token_ids = [tokenizer.encode(opt, add_special_tokens=False)[0] for opt in options]
                    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

                    batch_acts, batch_probs, batch_logits, batch_metrics = extractor.extract_batch(
                        inputs["input_ids"],
                        inputs["attention_mask"],
                        option_token_ids
                    )

                    for layer_idx, act in batch_acts[0].items():
                        direct_layer_acts[layer_idx].append(act)
                    direct_probs_list.append(batch_probs[0].tolist())
                    direct_logits_list.append(batch_logits[0].tolist())
                    for metric_name, metric_val in batch_metrics[0].items():
                        direct_metrics_lists[metric_name].append(metric_val)

                    del inputs

            if (batch_idx + 1) % 10 == 0:
                torch.cuda.empty_cache()

        # ============ PHASE 2: META PROMPTS ============
        print("\nProcessing meta prompts...")

        if META_TASK == "delegate":
            # Delegate task: process individually due to alternating mapping
            meta_layer_acts = {i: [] for i in range(num_layers)}
            meta_probs_list = []
            meta_entropies = []
            meta_responses = []
            meta_mappings = []
            meta_option_token_ids = [tokenizer.encode(opt, add_special_tokens=False)[0] for opt in meta_options]

            for trial_idx, q in enumerate(tqdm(questions, desc="Delegate prompts")):
                prompt, _, mapping = format_delegate_prompt(q, tokenizer, use_chat_template, trial_idx)
                inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

                batch_acts, batch_probs, batch_logits, batch_metrics = extractor.extract_batch(
                    inputs["input_ids"],
                    inputs["attention_mask"],
                    meta_option_token_ids
                )

                for layer_idx, act in batch_acts[0].items():
                    meta_layer_acts[layer_idx].append(act)
                meta_probs_list.append(batch_probs[0].tolist())
                meta_entropies.append(batch_metrics[0]["entropy"])
                meta_response = meta_options[np.argmax(batch_probs[0])]
                meta_responses.append(meta_response)
                meta_mappings.append(mapping)

                del inputs

                if (trial_idx + 1) % 50 == 0:
                    torch.cuda.empty_cache()
        else:
            # Confidence task: use prefix caching (all prompts share same prefix)
            meta_prompts = []
            meta_options_list = []
            for q in questions:
                prompt, opts = format_meta_prompt(q, tokenizer, use_chat_template)
                meta_prompts.append(prompt)
                meta_options_list.append(opts)

            # Use prefix caching for efficiency
            meta_acts, meta_probs_raw, _, meta_metrics_raw, meta_responses = process_prompts_with_prefix_cache(
                meta_prompts,
                meta_options_list,
                tokenizer,
                extractor,
                batch_size,
                desc="Meta prompts (with prefix cache)"
            )

            # Convert to expected format
            meta_layer_acts = {i: [] for i in range(num_layers)}
            for acts in meta_acts:
                for layer_idx, act in acts.items():
                    meta_layer_acts[layer_idx].append(act)

            meta_probs_list = [p.tolist() for p in meta_probs_raw]
            meta_entropies = [m["entropy"] for m in meta_metrics_raw]
            meta_mappings = [None] * len(questions)  # No mapping for confidence task

    finally:
        extractor.remove_hooks()

    # Convert to numpy arrays
    direct_activations = {
        layer_idx: np.array(acts) for layer_idx, acts in direct_layer_acts.items()
    }
    meta_activations = {
        layer_idx: np.array(acts) for layer_idx, acts in meta_layer_acts.items()
    }
    direct_metrics = {
        metric: np.array(values) for metric, values in direct_metrics_lists.items()
    }

    print(f"\nDirect activations shape (per layer): {direct_activations[0].shape}")
    print(f"Meta activations shape (per layer): {meta_activations[0].shape}")
    print(f"\nDirect uncertainty metrics:")
    for metric_name, values in direct_metrics.items():
        print(f"  {metric_name}: range=[{values.min():.3f}, {values.max():.3f}], "
              f"mean={values.mean():.3f}, std={values.std():.3f}")

    return {
        "direct_activations": direct_activations,
        "meta_activations": meta_activations,
        "direct_metrics": direct_metrics,
        "direct_probs": direct_probs_list,
        "direct_logits": direct_logits_list,
        "meta_entropies": np.array(meta_entropies),
        "meta_probs": meta_probs_list,
        "meta_responses": meta_responses,
        "meta_mappings": meta_mappings,
        "questions": questions
    }


def collect_meta_only(
    questions: List[Dict],
    model,
    tokenizer,
    num_layers: int,
    use_chat_template: bool,
    mc_data: Dict,
    batch_size: int = BATCH_SIZE
) -> Dict:
    """
    Collect only meta prompt data, reusing direct activations from mc_entropy_probe.py.

    This is much faster than collect_paired_data when MC data already exists.
    Uses KV cache prefix sharing for additional speedup on confidence task.
    """
    print(f"Collecting meta data only for {len(questions)} questions (reusing direct activations)...")

    extractor = BatchedExtractor(model, num_layers)
    extractor.register_hooks()

    model.eval()

    # Meta options depend on META_TASK
    meta_options = get_meta_options()

    try:
        if META_TASK == "delegate":
            # Delegate task: process individually due to alternating mapping
            meta_layer_acts = {i: [] for i in range(num_layers)}
            meta_probs_list = []
            meta_entropies = []
            meta_responses = []
            meta_mappings = []
            meta_option_token_ids = [tokenizer.encode(opt, add_special_tokens=False)[0] for opt in meta_options]

            for trial_idx, q in enumerate(tqdm(questions, desc="Delegate prompts")):
                prompt, _, mapping = format_delegate_prompt(q, tokenizer, use_chat_template, trial_idx)
                inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

                batch_acts, batch_probs, batch_logits, batch_metrics = extractor.extract_batch(
                    inputs["input_ids"],
                    inputs["attention_mask"],
                    meta_option_token_ids
                )

                for layer_idx, act in batch_acts[0].items():
                    meta_layer_acts[layer_idx].append(act)
                meta_probs_list.append(batch_probs[0].tolist())
                meta_entropies.append(batch_metrics[0]["entropy"])
                meta_response = meta_options[np.argmax(batch_probs[0])]
                meta_responses.append(meta_response)
                meta_mappings.append(mapping)

                del inputs

                if (trial_idx + 1) % 50 == 0:
                    torch.cuda.empty_cache()
        else:
            # Confidence task: use prefix caching (all prompts share same prefix)
            meta_prompts = []
            meta_options_list = []
            for q in questions:
                prompt, opts = format_meta_prompt(q, tokenizer, use_chat_template)
                meta_prompts.append(prompt)
                meta_options_list.append(opts)

            # Use prefix caching for efficiency
            meta_acts, meta_probs_raw, _, meta_metrics_raw, meta_responses = process_prompts_with_prefix_cache(
                meta_prompts,
                meta_options_list,
                tokenizer,
                extractor,
                batch_size,
                desc="Meta prompts (with prefix cache)"
            )

            # Convert to expected format
            meta_layer_acts = {i: [] for i in range(num_layers)}
            for acts in meta_acts:
                for layer_idx, act in acts.items():
                    meta_layer_acts[layer_idx].append(act)

            meta_probs_list = [p.tolist() for p in meta_probs_raw]
            meta_entropies = [m["entropy"] for m in meta_metrics_raw]
            meta_mappings = [None] * len(questions)

    finally:
        extractor.remove_hooks()

    meta_activations = {
        layer_idx: np.array(acts) for layer_idx, acts in meta_layer_acts.items()
    }

    print(f"Meta activations shape (per layer): {meta_activations[0].shape}")

    # Build direct data from mc_data
    # mc_data may have "direct_metrics" dict (new format) or just "direct_entropies" (old format)
    direct_probs_list = [m.get("probabilities", []) for m in mc_data["metadata"]]
    direct_logits_list = [m.get("logits", []) for m in mc_data["metadata"]]

    # Handle both old (entropies only) and new (all metrics) mc_data formats
    if "direct_metrics" in mc_data:
        direct_metrics = mc_data["direct_metrics"]
    else:
        # Old format: only has entropies, need to compute metrics from metadata
        direct_metrics = {metric: [] for metric in AVAILABLE_METRICS}
        for m in mc_data["metadata"]:
            probs = np.array(m.get("probabilities", []))
            logits = np.array(m.get("logits", [])) if m.get("logits") else None
            if len(probs) > 0:
                item_metrics = compute_uncertainty_metrics(probs, logits)
                for metric_name, metric_val in item_metrics.items():
                    direct_metrics[metric_name].append(metric_val)
        direct_metrics = {k: np.array(v) for k, v in direct_metrics.items()}

    return {
        "direct_activations": mc_data["direct_activations"],
        "meta_activations": meta_activations,
        "direct_metrics": direct_metrics,
        "direct_probs": direct_probs_list,
        "direct_logits": direct_logits_list,
        "meta_entropies": np.array(meta_entropies),
        "meta_probs": meta_probs_list,
        "meta_responses": meta_responses,
        "meta_mappings": meta_mappings,
        "questions": questions
    }


def collect_other_confidence(
    questions: List[Dict],
    model,
    tokenizer,
    num_layers: int,
    use_chat_template: bool,
    batch_size: int = BATCH_SIZE,
    collect_activations: bool = False
) -> Dict:
    """
    Collect other-confidence (human difficulty estimation) responses.

    This is a control task: asks model to estimate what % of college-educated
    people would know the answer (instead of asking about its own confidence).

    If the model is truly introspecting on its own uncertainty, the self-confidence
    task should correlate more strongly with its actual uncertainty metrics than
    this other-confidence task.

    Uses KV cache prefix sharing for efficiency.

    Args:
        collect_activations: If True, also collect layer activations for probe analysis.

    Returns dict with:
        - other_probs: list of prob arrays over S-Z options
        - other_responses: list of predicted confidence letters
        - other_signals: list of expected confidence values (weighted avg of midpoints)
        - other_activations: (optional) dict of layer_idx -> [n_questions, hidden_dim] activations
    """
    print(f"\nCollecting other-confidence (control) data for {len(questions)} questions...")

    extractor = BatchedExtractor(model, num_layers)
    extractor.register_hooks()

    model.eval()

    try:
        # Format all prompts
        other_prompts = []
        other_options_list = []
        for q in questions:
            prompt, opts = format_other_confidence_prompt(q, tokenizer, use_chat_template)
            other_prompts.append(prompt)
            other_options_list.append(opts)

        # Use prefix caching for efficiency
        desc = "Other-confidence (with activations)" if collect_activations else "Other-confidence (with prefix cache)"
        other_activations_raw, other_probs_raw, _, _, other_responses = process_prompts_with_prefix_cache(
            other_prompts,
            other_options_list,
            tokenizer,
            extractor,
            batch_size,
            desc=desc,
            collect_activations=collect_activations
        )

        # Convert to expected format and compute signals
        other_probs_list = [p.tolist() for p in other_probs_raw]
        other_signals = [get_other_confidence_signal(p) for p in other_probs_raw]

    finally:
        extractor.remove_hooks()

    print(f"Other-confidence signals: mean={np.mean(other_signals):.3f}, std={np.std(other_signals):.3f}")

    result = {
        "other_probs": other_probs_list,
        "other_responses": other_responses,
        "other_signals": np.array(other_signals),
    }

    if collect_activations and other_activations_raw is not None and len(other_activations_raw) > 0:
        # other_activations_raw is a list of dicts (one per question), each dict maps layer_idx -> activation
        # Convert to dict of numpy arrays keyed by layer
        num_layers = len(other_activations_raw[0])
        other_layer_acts = {i: [] for i in range(num_layers)}
        for acts in other_activations_raw:
            for layer_idx, act in acts.items():
                other_layer_acts[layer_idx].append(act)

        other_activations = {
            layer_idx: np.array(acts)
            for layer_idx, acts in other_layer_acts.items()
        }
        result["other_activations"] = other_activations
        print(f"Collected other-confidence activations: {len(other_activations)} layers, shape={other_activations[0].shape}")

    return result


# ============================================================================
# PROBE TRAINING AND EVALUATION
# ============================================================================

def extract_direction(
    scaler: StandardScaler,
    pca: Optional[PCA],
    probe: Ridge
) -> np.ndarray:
    """
    Extract normalized direction from trained probe in original activation space.

    Maps the probe weights back through PCA (if used) and standardization
    to get the direction in the original activation space.
    """
    coef = probe.coef_

    if pca is not None:
        # Map from PCA space back to scaled space
        direction_scaled = pca.components_.T @ coef
    else:
        direction_scaled = coef

    # Undo standardization scaling
    direction_original = direction_scaled / scaler.scale_

    # Normalize to unit length
    direction_original = direction_original / np.linalg.norm(direction_original)

    return direction_original


def train_probe(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    return_components: bool = False
) -> Dict:
    """
    Train a linear probe to predict entropy from activations.

    If return_components=True, also returns the scaler, pca, and probe objects
    for applying to new data.
    """
    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Apply PCA if enabled
    pca = None
    if USE_PCA:
        n_components = min(PCA_COMPONENTS, X_train.shape[0], X_train.shape[1])
        pca = PCA(n_components=n_components)
        X_train_final = pca.fit_transform(X_train_scaled)
        X_test_final = pca.transform(X_test_scaled)
    else:
        X_train_final = X_train_scaled
        X_test_final = X_test_scaled

    # Train Ridge regression probe
    probe = Ridge(alpha=PROBE_ALPHA)
    probe.fit(X_train_final, y_train)

    # Evaluate
    y_pred_train = probe.predict(X_train_final)
    y_pred_test = probe.predict(X_test_final)

    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    train_mae = mean_absolute_error(y_train, y_pred_train)
    test_mae = mean_absolute_error(y_test, y_pred_test)

    # NEW: Correlations
    test_pearson, _ = pearsonr(y_test, y_pred_test)
    test_spearman, _ = spearmanr(y_test, y_pred_test)

    result = {
        "train_r2": train_r2,
        "test_r2": test_r2,
        "train_mae": train_mae,
        "test_mae": test_mae,
        "test_pearson": test_pearson,   # <--- Added
        "test_spearman": test_spearman, # <--- Added
        "predictions": y_pred_test,
        "pca_variance_explained": pca.explained_variance_ratio_.sum() if USE_PCA else None
    }

    if return_components:
        result["scaler"] = scaler
        result["pca"] = pca
        result["probe"] = probe

    return result


def apply_trained_probe(
    X: np.ndarray,
    y: np.ndarray,
    scaler: StandardScaler,
    pca: Optional[PCA],
    probe: Ridge
) -> Dict:
    """Apply a pre-trained probe to new data using the original scaler."""
    X_scaled = scaler.transform(X)

    if pca is not None:
        X_final = pca.transform(X_scaled)
    else:
        X_final = X_scaled

    y_pred = probe.predict(X_final)

    r2 = r2_score(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    
    # NEW: Correlations - The "Shift Invariant" Check
    pearson, _ = pearsonr(y, y_pred)
    spearman, _ = spearmanr(y, y_pred)

    return {
        "r2": r2,
        "mae": mae,
        "pearson": pearson,     # <--- Added
        "spearman": spearman,   # <--- Added
        "predictions": y_pred
    }

def apply_probe_centering_only(
    X_meta: np.ndarray,
    y_test: np.ndarray,
    direct_scaler: StandardScaler,
    pca: Optional[PCA],
    probe: Ridge
) -> Dict:
    """
    Apply probe with Mean-Shift correction but Shared Variance.
    
    This fixes the 'Prompt Offset' (intercept) without distorting the 
    geometry (angles) of the activation space.
    """
    # 1. Calculate Meta Mean locally
    meta_mean = np.mean(X_meta, axis=0)
    
    # 2. Center Meta data using its OWN mean
    X_centered = X_meta - meta_mean
    
    # 3. Scale using DIRECT scaler's variance (preserve geometry)
    # direct_scaler.scale_ contains the std deviation of direct data
    X_scaled = X_centered / direct_scaler.scale_
    
    # 4. Apply Direct PCA and Probe
    if pca is not None:
        X_final = pca.transform(X_scaled)
    else:
        X_final = X_scaled
        
    y_pred = probe.predict(X_final)
    
    # Metrics
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    pearson, _ = pearsonr(y_test, y_pred)
    spearman, _ = spearmanr(y_test, y_pred)
    
    return {
        "r2": r2,
        "mae": mae,
        "pearson": pearson,
        "spearman": spearman,
        "predictions": y_pred
    }

def apply_probe_with_separate_scaling(
    X: np.ndarray,
    y: np.ndarray,
    pca: Optional[PCA],
    probe: Ridge
) -> Dict:
    """
    Apply a pre-trained probe to new data with SEPARATE standardization.
    
    Standardizes meta activations using their own statistics (Mean=0, Var=1).
    This acts as an upper bound / domain adaptation test.
    """
    # Standardize X using its own statistics (not the original scaler)
    new_scaler = StandardScaler()
    X_scaled = new_scaler.fit_transform(X)

    if pca is not None:
        X_final = pca.transform(X_scaled)
    else:
        X_final = X_scaled

    y_pred = probe.predict(X_final)

    r2 = r2_score(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    
    # NEW: Correlations
    pearson, _ = pearsonr(y, y_pred)
    spearman, _ = spearmanr(y, y_pred)

    return {
        "r2": r2,
        "mae": mae,
        "pearson": pearson,     # <--- Added
        "spearman": spearman,   # <--- Added
        "predictions": y_pred
    }

def run_introspection_analysis(
    direct_activations: Dict[int, np.ndarray],
    meta_activations: Dict[int, np.ndarray],
    direct_entropies: np.ndarray,
    pretrained_probe_path: Optional[str] = None,
    extract_directions: bool = True
) -> Dict:
    """
    Run the full introspection analysis:
    1. Train probe on direct activations -> direct entropy
    2. Test on held-out direct data (Sanity Check)
    3. Test on meta activations (The Transfer Tests):
       a. Shared Scaler (Exact probe application)
       b. Centered Scaler (Mean-shift correction, Shared geometry)
       c. Separate Scaler (Domain adaptation / Upper bound)
    4. Shuffled baseline control

    Returns (results, test_idx, directions).
    """
    print(f"\nRunning introspection analysis across {len(direct_activations)} layers...")

    # Split by question index (same split for both direct and meta)
    n_questions = len(direct_entropies)
    indices = np.arange(n_questions)
    train_idx, test_idx = train_test_split(
        indices,
        train_size=TRAIN_SPLIT,
        random_state=SEED
    )

    print(f"Train set: {len(train_idx)} questions, Test set: {len(test_idx)} questions")

    results = {}
    directions = {} if extract_directions else None
    probe_components = {}  # Store trained probe components for reuse

    for layer_idx in tqdm(sorted(direct_activations.keys()), desc="Training probes"):
        X_direct = direct_activations[layer_idx]
        X_meta = meta_activations[layer_idx]
        y = direct_entropies

        # Split
        X_direct_train = X_direct[train_idx]
        X_direct_test = X_direct[test_idx]
        X_meta_test = X_meta[test_idx]
        y_train = y[train_idx]
        y_test = y[test_idx]

        # 1. Train on direct, test on direct (sanity check)
        # Note: train_probe now returns correlations too
        direct_results = train_probe(
            X_direct_train, y_train,
            X_direct_test, y_test,
            return_components=True
        )

        # 2a. Original approach: Shared Scaler
        # Tests: Is the uncertainty vector identical in absolute terms?
        # (Likely to fail R2 due to offset, but should have high Pearson if introspecting)
        meta_results_shared = apply_trained_probe(
            X_meta_test, y_test,
            direct_results["scaler"],
            direct_results["pca"],
            direct_results["probe"]
        )

        # 2b. NEW: Centering Only
        # Tests: Is the uncertainty vector structurally identical (same angles) 
        # but shifted in position (prompt offset)?
        meta_results_centered = apply_probe_centering_only(
            X_meta_test, y_test,
            direct_results["scaler"],
            direct_results["pca"],
            direct_results["probe"]
        )

        # 2c. Fixed approach: Separate Scaler
        # Tests: Is the uncertainty vector recoverable if we allow warping the space?
        # (Upper bound / Domain adaptation)
        meta_results_separate = apply_probe_with_separate_scaling(
            X_meta_test, y_test,
            direct_results["pca"],
            direct_results["probe"]
        )

        # 3. Shuffled baseline
        shuffled_y_train = y_train.copy()
        np.random.shuffle(shuffled_y_train)
        shuffled_results = train_probe(
            X_direct_train, shuffled_y_train,
            X_direct_test, y_test,
            return_components=False
        )

        # 4. Train on meta, test on meta (does meta have ANY signal?)
        meta_to_meta_results = train_probe(
            X_meta[train_idx], y_train,
            X_meta_test, y_test,
            return_components=False
        )

        # 5. Extract entropy direction (for steering)
        if extract_directions:
            directions[layer_idx] = extract_direction(
                direct_results["scaler"],
                direct_results["pca"],
                direct_results["probe"]
            )

        # 6. Store probe components for reuse (e.g., other-confidence transfer)
        probe_components[layer_idx] = {
            "scaler": direct_results["scaler"],
            "pca": direct_results["pca"],
            "probe": direct_results["probe"],
        }

        # Pack results
        results[layer_idx] = {
            "direct_to_direct": {
                "train_r2": direct_results["train_r2"],
                "test_r2": direct_results["test_r2"],
                "test_pearson": direct_results["test_pearson"],
                "test_mae": direct_results["test_mae"],
                "predictions": direct_results["predictions"].tolist(),
            },
            "direct_to_meta": {
                # Shared scaler (Original)
                "r2": meta_results_shared["r2"],
                "pearson": meta_results_shared["pearson"],
                "mae": meta_results_shared["mae"],
                "predictions": meta_results_shared["predictions"].tolist(),
            },
            "direct_to_meta_centered": {
                # Centered scaler (New "Goldilocks" metric)
                "r2": meta_results_centered["r2"],
                "pearson": meta_results_centered["pearson"],
                "mae": meta_results_centered["mae"],
                "predictions": meta_results_centered["predictions"].tolist(),
            },
            "direct_to_meta_fixed": {
                # Separate scaler (Separate mean+var)
                "r2": meta_results_separate["r2"],
                "pearson": meta_results_separate["pearson"],
                "mae": meta_results_separate["mae"],
                "predictions": meta_results_separate["predictions"].tolist(),
            },
            "shuffled_baseline": {
                "r2": shuffled_results["test_r2"],
                "mae": shuffled_results["test_mae"],
            },
            "meta_to_meta": {
                "train_r2": meta_to_meta_results["train_r2"],
                "test_r2": meta_to_meta_results["test_r2"],
            },
            "pca_variance_explained": direct_results["pca_variance_explained"]
        }

    return results, test_idx, directions, probe_components

def run_mc_answer_analysis(
    direct_activations: Dict[int, np.ndarray],
    meta_activations: Dict[int, np.ndarray],
    model_predicted_answer: np.ndarray,
    train_idx: np.ndarray,
    test_idx: np.ndarray,
) -> Dict:
    """
    Train logistic regression probe to predict model's MC answer choice (A/B/C/D).
    Computes BOTH Separate (Upper Bound) and Centered (Rigorous) transfer metrics.
    """
    print(f"\nRunning MC answer probe analysis across {len(direct_activations)} layers...")

    results = {}
    mc_probe_components = {}  # Store trained probe components for reuse

    for layer_idx in tqdm(sorted(direct_activations.keys()), desc="Training MC answer probes"):
        X_direct = direct_activations[layer_idx]
        X_meta = meta_activations[layer_idx]
        y = model_predicted_answer

        # Split
        X_direct_train = X_direct[train_idx]
        X_direct_test = X_direct[test_idx]
        X_meta_test = X_meta[test_idx]
        y_train = y[train_idx]
        y_test = y[test_idx]

        # --- Train on Direct ---
        scaler = StandardScaler()
        X_direct_train_scaled = scaler.fit_transform(X_direct_train)
        X_direct_test_scaled = scaler.transform(X_direct_test)

        # PCA
        pca = PCA(n_components=min(256, X_direct_train_scaled.shape[1], X_direct_train_scaled.shape[0]))
        X_direct_train_pca = pca.fit_transform(X_direct_train_scaled)
        X_direct_test_pca = pca.transform(X_direct_test_scaled)

        # Logistic Regression
        clf = LogisticRegression(max_iter=1000, random_state=42)
        clf.fit(X_direct_train_pca, y_train)

        # D2D Accuracy
        d2d_accuracy = clf.score(X_direct_test_pca, y_test)
        d2d_predictions = clf.predict(X_direct_test_pca)

        # --- Transfer 1: Separate Scaling (Upper Bound) ---
        meta_scaler_sep = StandardScaler()
        X_meta_test_sep = meta_scaler_sep.fit_transform(X_meta_test)
        X_meta_test_sep_pca = pca.transform(X_meta_test_sep)
        d2m_separate_accuracy = clf.score(X_meta_test_sep_pca, y_test)

        # --- Transfer 2: Centered Scaling (Rigorous) ---
        # 1. Center Meta using its own mean
        meta_mean = np.mean(X_meta_test, axis=0)
        X_meta_centered = X_meta_test - meta_mean
        # 2. Scale using DIRECT variance (scaler.scale_)
        # This ensures we aren't "re-fitting" the geometry
        X_meta_test_cen = X_meta_centered / scaler.scale_
        X_meta_test_cen_pca = pca.transform(X_meta_test_cen)
        d2m_centered_accuracy = clf.score(X_meta_test_cen_pca, y_test)

        # Shuffled baseline
        shuffled_y_train = y_train.copy()
        np.random.shuffle(shuffled_y_train)
        clf_shuffled = LogisticRegression(max_iter=1000, random_state=42)
        clf_shuffled.fit(X_direct_train_pca, shuffled_y_train)
        shuffled_accuracy = clf_shuffled.score(X_direct_test_pca, y_test)

        results[layer_idx] = {
            "d2d_accuracy": d2d_accuracy,
            "d2m_accuracy": d2m_separate_accuracy, # Legacy key
            "d2m_separate_accuracy": d2m_separate_accuracy,
            "d2m_centered_accuracy": d2m_centered_accuracy,
            "shuffled_accuracy": shuffled_accuracy,
            "d2d_predictions": d2d_predictions.tolist(),
        }

        # Store probe components for reuse (e.g., other-confidence transfer)
        mc_probe_components[layer_idx] = {
            "scaler": scaler,
            "pca": pca,
            "clf": clf,
        }

    return results, mc_probe_components


def apply_probes_to_other(
    other_activations: Dict[int, np.ndarray],
    entropy_probe_components: Dict[int, Dict],
    mc_probe_components: Dict[int, Dict],
    direct_entropy: np.ndarray,
    model_predicted_answer: np.ndarray,
    test_idx: np.ndarray,
) -> Dict:
    """
    Apply trained probes to other-confidence activations.

    Only computes D→M(Other) - reuses trained probe components from main analysis.
    This is a control test: if transfer works equally well to other-confidence,
    the model encodes uncertainty similarly regardless of the meta task.

    Args:
        other_activations: Layer activations from other-confidence task
        entropy_probe_components: {layer: {"scaler", "pca", "probe"}} from run_introspection_analysis
        mc_probe_components: {layer: {"scaler", "pca", "clf"}} from run_mc_answer_analysis
        direct_entropy: Ground truth entropy values
        model_predicted_answer: Model's predicted MC answer (A/B/C/D as int)
        test_idx: Test set indices

    Returns:
        Dict with layer-wise D→M(Other) R² and accuracy for both probes
    """
    print(f"\nApplying trained probes to other-confidence activations...")

    y_entropy = direct_entropy[test_idx]
    y_mc = model_predicted_answer[test_idx]

    results = {}
    for layer_idx in tqdm(sorted(other_activations.keys()), desc="Testing D→M(Other) transfer"):
        X_other = other_activations[layer_idx][test_idx]

        # --- Entropy probe: centered scaling ---
        ent_comps = entropy_probe_components[layer_idx]
        other_mean = np.mean(X_other, axis=0)
        X_other_centered = X_other - other_mean
        X_other_scaled = X_other_centered / ent_comps["scaler"].scale_
        X_other_pca = ent_comps["pca"].transform(X_other_scaled)
        entropy_preds = ent_comps["probe"].predict(X_other_pca)
        entropy_r2 = r2_score(y_entropy, entropy_preds)

        # --- MC answer probe: centered scaling ---
        mc_comps = mc_probe_components[layer_idx]
        X_other_mc_centered = X_other - np.mean(X_other, axis=0)
        X_other_mc_scaled = X_other_mc_centered / mc_comps["scaler"].scale_
        X_other_mc_pca = mc_comps["pca"].transform(X_other_mc_scaled)
        mc_acc = mc_comps["clf"].score(X_other_mc_pca, y_mc)

        results[layer_idx] = {
            "d2m_other_entropy_r2": entropy_r2,
            "d2m_other_mc_accuracy": mc_acc,
        }

    return results


def compute_calibration_masks(
    stated_confidence: np.ndarray,
    direct_metric: np.ndarray,
    metric_is_uncertainty: bool = True
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Compute calibrated/miscalibrated masks using median split.

    Args:
        stated_confidence: Model's stated confidence (meta-judgment)
        direct_metric: The uncertainty/confidence metric being probed
        metric_is_uncertainty: True if high metric = uncertain (entropy),
                               False if high metric = confident (logit_gap, top_prob, margin, top_logit)

    Calibrated: model's meta-judgment matches actual uncertainty
      - For uncertainty metrics (entropy): high conf + low metric, or low conf + high metric
      - For confidence metrics (logit_gap): high conf + high metric, or low conf + low metric

    Miscalibrated: model's meta-judgment opposes actual uncertainty

    Returns (calibrated_mask, miscalibrated_mask).
    Uses median split for balanced groups.
    """
    conf_median = np.median(stated_confidence)
    metric_median = np.median(direct_metric)

    if metric_is_uncertainty:
        # High metric = uncertain, so calibrated = (high conf + low metric) or (low conf + high metric)
        calibrated = (
            ((stated_confidence > conf_median) & (direct_metric < metric_median)) |
            ((stated_confidence < conf_median) & (direct_metric > metric_median))
        )
        miscalibrated = (
            ((stated_confidence > conf_median) & (direct_metric > metric_median)) |
            ((stated_confidence < conf_median) & (direct_metric < metric_median))
        )
    else:
        # High metric = confident, so calibrated = (high conf + high metric) or (low conf + low metric)
        calibrated = (
            ((stated_confidence > conf_median) & (direct_metric > metric_median)) |
            ((stated_confidence < conf_median) & (direct_metric < metric_median))
        )
        miscalibrated = (
            ((stated_confidence > conf_median) & (direct_metric < metric_median)) |
            ((stated_confidence < conf_median) & (direct_metric > metric_median))
        )

    return calibrated, miscalibrated


def split_results_by_calibration(
    results: Dict,
    y_test: np.ndarray,
    calibrated_mask: np.ndarray,
    miscalibrated_mask: np.ndarray
) -> Dict:
    """
    Recompute R² separately for calibrated/miscalibrated test trials.

    Uses predictions already stored in results dict from run_introspection_analysis.
    Also extracts shuffled baseline R² for reference.
    """
    split_results = {}
    for layer_idx, layer_data in results.items():
        d2d_preds = np.array(layer_data["direct_to_direct"]["predictions"])
        d2m_preds = np.array(layer_data["direct_to_meta_fixed"]["predictions"])
        shuffled_r2 = layer_data["shuffled_baseline"]["r2"]

        # Calibrated subset
        if calibrated_mask.sum() > 1:
            d2d_r2_cal = r2_score(y_test[calibrated_mask], d2d_preds[calibrated_mask])
            d2m_r2_cal = r2_score(y_test[calibrated_mask], d2m_preds[calibrated_mask])
        else:
            d2d_r2_cal = float('nan')
            d2m_r2_cal = float('nan')

        # Miscalibrated subset
        if miscalibrated_mask.sum() > 1:
            d2d_r2_mis = r2_score(y_test[miscalibrated_mask], d2d_preds[miscalibrated_mask])
            d2m_r2_mis = r2_score(y_test[miscalibrated_mask], d2m_preds[miscalibrated_mask])
        else:
            d2d_r2_mis = float('nan')
            d2m_r2_mis = float('nan')

        split_results[layer_idx] = {
            "calibrated": {"d2d_r2": d2d_r2_cal, "d2m_r2": d2m_r2_cal},
            "miscalibrated": {"d2d_r2": d2d_r2_mis, "d2m_r2": d2m_r2_mis},
            "shuffled_r2": shuffled_r2,
        }
    return split_results


def plot_calibration_split(
    split_results: Dict,
    n_calibrated: int,
    n_miscalibrated: int,
    output_path: str
):
    """
    Side-by-side 1x2 figure: calibrated (left) vs miscalibrated (right).

    Shows D2D and D2M R² curves for each subset to see if transfer
    is driven by calibrated or miscalibrated trials.
    """
    layers = sorted(split_results.keys())
    shuffled_r2 = [split_results[l]["shuffled_r2"] for l in layers]

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Left: Calibrated trials
    ax1 = axes[0]
    cal_d2d = [split_results[l]["calibrated"]["d2d_r2"] for l in layers]
    cal_d2m = [split_results[l]["calibrated"]["d2m_r2"] for l in layers]
    ax1.plot(layers, cal_d2d, 'o-', label='Direct→Direct', linewidth=2, color='C0')
    ax1.plot(layers, cal_d2m, 's-', label='Direct→Meta', linewidth=2, color='C1')
    ax1.plot(layers, shuffled_r2, 'x--', label='Shuffled baseline', linewidth=1, alpha=0.5, color='gray')
    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax1.set_xlabel('Layer Index')
    ax1.set_ylabel('R² Score')
    ax1.set_title(f'Calibrated Trials (n={n_calibrated})')
    ax1.legend(loc='best')
    ax1.grid(True, alpha=0.3)

    # Right: Miscalibrated trials
    ax2 = axes[1]
    mis_d2d = [split_results[l]["miscalibrated"]["d2d_r2"] for l in layers]
    mis_d2m = [split_results[l]["miscalibrated"]["d2m_r2"] for l in layers]
    ax2.plot(layers, mis_d2d, 'o-', label='Direct→Direct', linewidth=2, color='C0')
    ax2.plot(layers, mis_d2m, 's-', label='Direct→Meta', linewidth=2, color='C1')
    ax2.plot(layers, shuffled_r2, 'x--', label='Shuffled baseline', linewidth=1, alpha=0.5, color='gray')
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax2.set_xlabel('Layer Index')
    ax2.set_ylabel('R² Score')
    ax2.set_title(f'Miscalibrated Trials (n={n_miscalibrated})')
    ax2.legend(loc='best')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Calibration split plot saved to {output_path}")


def plot_other_confidence_comparison(
    main_results: Dict,
    mc_results: Dict,
    other_results: Dict,
    output_path: str
):
    """
    Compare direct-trained probe transfer to self-confidence vs other-confidence.

    2-panel figure:
      - Left: Entropy probe (D→D, D→M Self, D→M Other, Shuffled)
      - Right: MC answer probe (D→D, D→M Self, D→M Other, Shuffled)

    If D→M(Other) transfer is as strong as D→M(Self), it suggests the model
    encodes uncertainty similarly across both meta tasks, not specifically
    during self-confidence introspection.
    """
    layers = sorted(main_results.keys())

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # --- Panel 1: Entropy Probe ---
    ax1 = axes[0]
    d2d = [main_results[l]["direct_to_direct"]["test_r2"] for l in layers]
    d2m_self = [main_results[l]["direct_to_meta_fixed"]["r2"] for l in layers]
    d2m_other = [other_results[l]["d2m_other_entropy_r2"] for l in layers]
    shuffled = [main_results[l]["shuffled_baseline"]["r2"] for l in layers]

    ax1.plot(layers, d2d, 'o-', label='D→D', linewidth=2, color='C0')
    ax1.plot(layers, d2m_self, 's-', label='D→M (Self)', linewidth=2, color='C1')
    ax1.plot(layers, d2m_other, '^-', label='D→M (Other)', linewidth=2, color='C2')
    ax1.plot(layers, shuffled, 'x--', label='Shuffled', linewidth=1, alpha=0.5, color='gray')
    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax1.set_xlabel('Layer Index')
    ax1.set_ylabel('R² Score')
    ax1.set_title('Entropy Probe Transfer')
    ax1.legend(loc='best')
    ax1.grid(True, alpha=0.3)

    # --- Panel 2: MC Answer Probe ---
    ax2 = axes[1]
    mc_d2d = [mc_results[l]["d2d_accuracy"] for l in layers]
    mc_d2m_self = [mc_results[l]["d2m_centered_accuracy"] for l in layers]
    mc_d2m_other = [other_results[l]["d2m_other_mc_accuracy"] for l in layers]
    mc_shuffled = [mc_results[l]["shuffled_accuracy"] for l in layers]

    ax2.plot(layers, mc_d2d, 'o-', label='D→D', linewidth=2, color='C0')
    ax2.plot(layers, mc_d2m_self, 's-', label='D→M (Self)', linewidth=2, color='C1')
    ax2.plot(layers, mc_d2m_other, '^-', label='D→M (Other)', linewidth=2, color='C2')
    ax2.plot(layers, mc_shuffled, 'x--', label='Shuffled', linewidth=1, alpha=0.5, color='gray')
    ax2.axhline(y=0.25, color='black', linestyle='-', alpha=0.3)  # 4-way random chance
    ax2.set_xlabel('Layer Index')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('MC Answer Probe Transfer')
    ax2.legend(loc='best')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Other-confidence transfer comparison saved to {output_path}")


# ============================================================================
# ANALYSIS AND VISUALIZATION
# ============================================================================

def analyze_behavioral_introspection(
    meta_responses: List[str],
    direct_entropies: np.ndarray,
    test_idx: np.ndarray,
    meta_probs: List[List[float]] = None,
    meta_mappings: List[Dict[str, str]] = None,
    direct_probs: List[List[float]] = None,
    questions: List[Dict] = None
) -> Dict:
    """
    Analyze correlation between model's stated confidence and direct entropy.

    If the model is introspecting, its stated confidence should negatively
    correlate with direct entropy (high confidence = low entropy).

    For delegate task, confidence = P(Answer) from meta_probs.
    For confidence task, confidence = weighted average of midpoints (expected value).

    For delegate task, also computes:
    - Delegation rate: fraction of questions delegated
    - Self-answer accuracy: accuracy when model chooses to answer itself
    - Teammate accuracy when delegated (fixed at 50%)
    """
    # Convert meta responses to confidence values
    # For confidence task, use weighted average of midpoints (expected value)
    # For delegate task, use P(Answer)
    if META_TASK == "delegate":
        stated_confidence = np.array([
            local_response_to_confidence(r, np.array(p) if p else None, m)
            for r, p, m in zip(
                meta_responses,
                meta_probs or [None] * len(meta_responses),
                meta_mappings or [None] * len(meta_responses)
            )
        ])
    else:
        # Confidence task: weighted average of midpoints
        from tasks import get_stated_confidence_signal
        stated_confidence = np.array([
            get_stated_confidence_signal(np.array(p)) if p else 0.5
            for p in meta_probs
        ])

    # Split into test set (same split used for probe evaluation)
    test_confidence = stated_confidence[test_idx]
    test_entropy = direct_entropies[test_idx]

    # Subsample-to-m interval approach for CIs
    # This gives intervals calibrated to test size, centered on point estimates
    from scipy import stats
    n_subsamples = 200  # K iterations
    n = len(direct_entropies)
    m = len(test_idx)  # test size

    # Helper: Fisher z transform and inverse
    def fisher_z(r):
        # Clip to avoid inf at r=±1
        r = np.clip(r, -0.9999, 0.9999)
        return 0.5 * np.log((1 + r) / (1 - r))

    def fisher_z_inv(z):
        return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)

    # Full correlation (point estimate on all data - stable)
    full_r = np.corrcoef(stated_confidence, direct_entropies)[0, 1]
    full_z = fisher_z(full_r)

    # Test correlation (point estimate on specific test_idx)
    test_r = np.corrcoef(test_confidence, test_entropy)[0, 1]
    n_test = len(test_confidence)

    # Subsample-to-m interval for FULL correlation
    # Subsample m items, compute correlation, get deviation from full_r in z-space
    full_subsample_deviations = []
    for k in range(n_subsamples):
        rng = np.random.default_rng(k)
        idx = rng.choice(n, size=m, replace=False)
        sub_r = np.corrcoef(stated_confidence[idx], direct_entropies[idx])[0, 1]
        if not np.isnan(sub_r):
            sub_z = fisher_z(sub_r)
            full_subsample_deviations.append(sub_z - full_z)

    # Get percentiles of deviations, then map back to correlation space
    dev_lower = np.percentile(full_subsample_deviations, 2.5)
    dev_upper = np.percentile(full_subsample_deviations, 97.5)
    full_ci_lower = fisher_z_inv(full_z + dev_lower)
    full_ci_upper = fisher_z_inv(full_z + dev_upper)
    full_ci_std = np.std([fisher_z_inv(full_z + d) for d in full_subsample_deviations])

    # Subsample-to-m interval for TEST correlation (random test subsets)
    # This shows how test correlation varies across different random splits
    test_subsample_correlations = []
    for k in range(n_subsamples):
        rng = np.random.default_rng(k)
        idx = rng.choice(n, size=m, replace=False)
        sub_r = np.corrcoef(stated_confidence[idx], direct_entropies[idx])[0, 1]
        if not np.isnan(sub_r):
            test_subsample_correlations.append(sub_r)

    test_ci_mean = np.mean(test_subsample_correlations)
    test_ci_std = np.std(test_subsample_correlations)
    test_ci_lower = np.percentile(test_subsample_correlations, 2.5)
    test_ci_upper = np.percentile(test_subsample_correlations, 97.5)

    # P-value consistent with subsample-to-m CI
    # Compute from the subsample distribution: what fraction of shifted subsamples cross 0?
    # Under the null, the true correlation is 0, so we ask: if we shift our distribution
    # so full_r maps to 0, what fraction of subsamples would be on the opposite side?
    # This is equivalent to asking: does the CI include 0?
    if len(full_subsample_deviations) > 0:
        # The subsample correlations are: fisher_z_inv(full_z + deviation)
        # We want to know the probability that the true correlation is 0
        # Using the percentile method: p-value = 2 * min(fraction below 0, fraction above 0)
        subsample_correlations = [fisher_z_inv(full_z + d) for d in full_subsample_deviations]
        n_below_zero = sum(1 for r in subsample_correlations if r < 0)
        n_above_zero = sum(1 for r in subsample_correlations if r > 0)
        n_total = len(subsample_correlations)
        # Two-tailed p-value
        tail_fraction = min(n_below_zero, n_above_zero) / n_total
        full_pvalue = 2 * tail_fraction
        # Ensure p-value is at least 1/n_subsamples (can't be exactly 0 with finite samples)
        full_pvalue = max(full_pvalue, 1 / n_subsamples)
    else:
        full_pvalue = np.nan

    if abs(test_r) < 1 and n_test > 2:
        t_stat = test_r * np.sqrt((n_test - 2) / (1 - test_r**2))
        test_pvalue = 2 * (1 - stats.t.cdf(abs(t_stat), df=n_test-2))
    else:
        test_pvalue = np.nan

    # Partial correlation (always computed - equals Pearson when no controls)
    import pandas as pd
    from logres_helpers import partial_correlation_on_decision

    # Control variables for partial correlation (currently empty, ready for future use)
    control_series_list = []

    full_partial_result = partial_correlation_on_decision(
        dv_series=pd.Series(direct_entropies, name='entropy'),
        iv_series=pd.Series(stated_confidence, name='confidence'),
        control_series_list=control_series_list
    )

    result = {
        "full_correlation": float(full_r),
        "full_correlation_pvalue": float(full_pvalue),
        "test_correlation": float(test_r),
        "test_correlation_pvalue": float(test_pvalue),
        # Subsample-to-m interval for full: centered on full_r, width calibrated to test size
        "full_correlation_ci95": [float(full_ci_lower), float(full_ci_upper)],
        "full_correlation_ci_std": float(full_ci_std),
        # Subsample-to-m interval for test: distribution of correlations at test size
        "test_correlation_ci95": [float(test_ci_lower), float(test_ci_upper)],
        "test_correlation_ci_mean": float(test_ci_mean),
        "test_correlation_ci_std": float(test_ci_std),
        "n_subsamples": n_subsamples,
        # Partial correlation (equals Pearson when no controls)
        "partial_correlation": float(full_partial_result['correlation']),
        "partial_correlation_ci95": [float(full_partial_result['ci_lower']), float(full_partial_result['ci_upper'])],
        "partial_correlation_pvalue": float(full_partial_result['p_value']),
        "partial_correlation_controls": [s.name for s in control_series_list],
        "n_samples_full": n,
        "n_samples_test": n_test,
        "test_confidence_mean": float(test_confidence.mean()),
        "test_confidence_std": float(test_confidence.std()),
        "test_entropy_mean": float(test_entropy.mean()),
        "test_entropy_std": float(test_entropy.std()),
    }

    # Delegate-specific metrics
    if META_TASK == "delegate" and meta_mappings is not None:
        # Determine delegation decisions based on response and mapping
        delegated = []
        self_answers = []
        for i, (response, mapping) in enumerate(zip(meta_responses, meta_mappings)):
            if mapping is not None:
                decision = mapping.get(response, "Unknown")
                is_delegated = (decision == "Delegate")
                delegated.append(is_delegated)
                if not is_delegated:
                    self_answers.append(i)

        delegation_rate = sum(delegated) / len(delegated) if delegated else 0.0
        result["delegation_rate"] = float(delegation_rate)
        result["num_delegated"] = sum(delegated)
        result["num_self_answered"] = len(self_answers)

        # Compute self-answer accuracy if we have the data
        if direct_probs is not None and questions is not None and self_answers:
            self_correct = 0
            for idx in self_answers:
                if idx < len(direct_probs) and idx < len(questions):
                    probs = direct_probs[idx]
                    q = questions[idx]
                    if probs and "correct_answer" in q and "options" in q:
                        options = list(q["options"].keys())
                        predicted_answer = options[np.argmax(probs)]
                        if predicted_answer == q["correct_answer"]:
                            self_correct += 1

            self_answer_accuracy = self_correct / len(self_answers)
            result["self_answer_accuracy"] = float(self_answer_accuracy)
            result["self_correct"] = self_correct

            # Teammate accuracy is fixed at 50% (by design of the game)
            result["teammate_accuracy"] = 0.5

            # Team score: self-answered correct + delegated * 0.5
            team_score = self_correct + sum(delegated) * 0.5
            result["team_score"] = float(team_score)
            result["team_score_normalized"] = float(team_score / len(delegated)) if delegated else 0.0

    # Include stated_confidence for downstream analysis (e.g., calibration split)
    result["stated_confidence"] = stated_confidence.tolist()

    return result


def analyze_other_confidence_control(
    other_signals: np.ndarray,
    self_confidence: np.ndarray,
    direct_metric: np.ndarray,
    test_idx: np.ndarray
) -> Dict:
    """
    Analyze other-confidence (human difficulty estimation) as a control.

    Compares:
    1. Correlation of self-confidence vs direct metric (introspection)
    2. Correlation of other-confidence vs direct metric (control)
    3. Correlation between self and other confidence

    If the model is truly introspecting, self-confidence should correlate
    more strongly with its own uncertainty than other-confidence does.
    """
    from scipy import stats

    n = len(direct_metric)
    n_test = len(test_idx)

    # Helper: Fisher z transform
    def fisher_z(r):
        r = np.clip(r, -0.9999, 0.9999)
        return 0.5 * np.log((1 + r) / (1 - r))

    def fisher_z_inv(z):
        return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)

    # 1. Self-confidence vs direct metric (already computed in behavioral analysis,
    #    but we recompute for completeness and to use for comparison)
    self_r = np.corrcoef(self_confidence, direct_metric)[0, 1]
    if abs(self_r) < 1 and n > 2:
        t_stat = self_r * np.sqrt((n - 2) / (1 - self_r**2))
        self_pvalue = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-2))
    else:
        self_pvalue = np.nan

    # 2. Other-confidence vs direct metric
    other_r = np.corrcoef(other_signals, direct_metric)[0, 1]
    if abs(other_r) < 1 and n > 2:
        t_stat = other_r * np.sqrt((n - 2) / (1 - other_r**2))
        other_pvalue = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-2))
    else:
        other_pvalue = np.nan

    # 3. Self vs Other confidence correlation
    self_other_r = np.corrcoef(self_confidence, other_signals)[0, 1]
    if abs(self_other_r) < 1 and n > 2:
        t_stat = self_other_r * np.sqrt((n - 2) / (1 - self_other_r**2))
        self_other_pvalue = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-2))
    else:
        self_other_pvalue = np.nan

    # 4. Compare correlations: is self-confidence significantly more correlated
    #    with direct metric than other-confidence?
    # Use Steiger's Z-test for comparing dependent correlations
    # (self vs metric) vs (other vs metric), where self and other are correlated
    r12 = self_r  # self vs metric
    r13 = other_r  # other vs metric
    r23 = self_other_r  # self vs other

    # Steiger's Z formula for comparing dependent correlations
    if abs(r12) < 1 and abs(r13) < 1 and abs(r23) < 1:
        # Average correlation
        r_avg = (r12 + r13) / 2

        # Hotelling-Williams t-test approximation
        f = (1 - r23) / (2 * (1 - r_avg**2)) if abs(r_avg) < 1 else 1
        det = 1 - r12**2 - r13**2 - r23**2 + 2*r12*r13*r23

        if det > 0:
            t_stat = (r12 - r13) * np.sqrt((n - 3) * (1 + r23) / (2 * det))
            diff_pvalue = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-3))
        else:
            diff_pvalue = np.nan
    else:
        diff_pvalue = np.nan

    # Bootstrap CI for the difference in correlations
    n_bootstrap = 1000
    diff_samples = []
    for b in range(n_bootstrap):
        rng = np.random.default_rng(b)
        idx = rng.choice(n, size=n, replace=True)
        self_r_b = np.corrcoef(self_confidence[idx], direct_metric[idx])[0, 1]
        other_r_b = np.corrcoef(other_signals[idx], direct_metric[idx])[0, 1]
        if not np.isnan(self_r_b) and not np.isnan(other_r_b):
            diff_samples.append(self_r_b - other_r_b)

    if diff_samples:
        diff_mean = np.mean(diff_samples)
        diff_ci_lower = np.percentile(diff_samples, 2.5)
        diff_ci_upper = np.percentile(diff_samples, 97.5)
    else:
        diff_mean = self_r - other_r
        diff_ci_lower = diff_ci_upper = np.nan

    return {
        # Self-confidence analysis
        "self_vs_metric_r": float(self_r),
        "self_vs_metric_pvalue": float(self_pvalue),
        # Other-confidence analysis
        "other_vs_metric_r": float(other_r),
        "other_vs_metric_pvalue": float(other_pvalue),
        # Self vs Other correlation
        "self_vs_other_r": float(self_other_r),
        "self_vs_other_pvalue": float(self_other_pvalue),
        # Comparison
        "correlation_difference": float(self_r - other_r),
        "correlation_difference_ci95": [float(diff_ci_lower), float(diff_ci_upper)],
        "correlation_difference_pvalue": float(diff_pvalue) if not np.isnan(diff_pvalue) else None,
        # Descriptives
        "self_confidence_mean": float(self_confidence.mean()),
        "self_confidence_std": float(self_confidence.std()),
        "other_confidence_mean": float(other_signals.mean()),
        "other_confidence_std": float(other_signals.std()),
        "n_samples": n,
    }


def print_results(results: Dict, behavioral: Dict, other_confidence_analysis: Dict = None):
    """Print summary of results."""
    print("\n" + "=" * 100)
    print("INTROSPECTION EXPERIMENT RESULTS")
    print("=" * 100)

    print("\n--- Behavioral Analysis ---")
    n_full = behavioral.get('n_samples_full', '?')
    n_test = behavioral.get('n_samples_test', '?')
    n_subsamples = behavioral.get('n_subsamples', '?')
    print(f"Correlation (stated confidence vs direct entropy):")

    # Full dataset correlation with subsample-to-m CI
    full_p = behavioral.get('full_correlation_pvalue')
    p_str = f", p = {full_p:.2e}" if full_p is not None and not np.isnan(full_p) else ""
    full_ci = behavioral.get('full_correlation_ci95', [None, None])
    full_ci_std = behavioral.get('full_correlation_ci_std')
    if full_ci[0] is not None:
        print(f"  Full  (n={n_full}):  r = {behavioral['full_correlation']:.4f} ± {full_ci_std:.4f}  [95% CI: {full_ci[0]:.4f}, {full_ci[1]:.4f}]{p_str}")
    else:
        print(f"  Full  (n={n_full}):  r = {behavioral['full_correlation']:.4f}{p_str}")

    # Test set correlation with subsample CI
    test_p = behavioral.get('test_correlation_pvalue')
    p_str = f", p = {test_p:.2e}" if test_p is not None and not np.isnan(test_p) else ""
    test_ci = behavioral.get('test_correlation_ci95', [None, None])
    test_ci_std = behavioral.get('test_correlation_ci_std')
    if test_ci[0] is not None:
        print(f"  Test  (n={n_test}):  r = {behavioral['test_correlation']:.4f} ± {test_ci_std:.4f}  [95% CI: {test_ci[0]:.4f}, {test_ci[1]:.4f}]{p_str}")
    else:
        print(f"  Test  (n={n_test}):  r = {behavioral['test_correlation']:.4f}{p_str}")

    print(f"  (CIs from {n_subsamples} subsamples to test size, centered on point estimate)")

    # Partial correlation
    partial_r = behavioral.get('partial_correlation')
    partial_ci = behavioral.get('partial_correlation_ci95', [None, None])
    partial_p = behavioral.get('partial_correlation_pvalue')
    controls = behavioral.get('partial_correlation_controls', [])
    if partial_r is not None:
        p_str = f", p = {partial_p:.2e}" if partial_p is not None and not np.isnan(partial_p) else ""
        ctrl_str = f" (controlling for {', '.join(controls)})" if controls else ""
        print(f"  Partial{ctrl_str}: r = {partial_r:.4f}  [95% CI: {partial_ci[0]:.4f}, {partial_ci[1]:.4f}]{p_str}")

    print(f"  (Negative correlation suggests introspection; positive suggests miscalibration)")

    # Other-confidence control analysis (only for confidence task)
    if other_confidence_analysis is not None:
        print("\n--- Other-Confidence Control (Human Difficulty Estimation) ---")
        self_r = other_confidence_analysis['self_vs_metric_r']
        other_r = other_confidence_analysis['other_vs_metric_r']
        diff = other_confidence_analysis['correlation_difference']
        diff_ci = other_confidence_analysis['correlation_difference_ci95']
        diff_p = other_confidence_analysis.get('correlation_difference_pvalue')

        self_p = other_confidence_analysis['self_vs_metric_pvalue']
        other_p = other_confidence_analysis['other_vs_metric_pvalue']

        self_p_str = f", p = {self_p:.2e}" if self_p is not None and not np.isnan(self_p) else ""
        other_p_str = f", p = {other_p:.2e}" if other_p is not None and not np.isnan(other_p) else ""

        print(f"  Self-confidence vs {METRIC}:    r = {self_r:.4f}{self_p_str}")
        print(f"  Other-confidence vs {METRIC}:   r = {other_r:.4f}{other_p_str}")
        print(f"  Self vs Other confidence:       r = {other_confidence_analysis['self_vs_other_r']:.4f}")
        print(f"")
        print(f"  Difference (self - other):      Δr = {diff:.4f}  [95% CI: {diff_ci[0]:.4f}, {diff_ci[1]:.4f}]")

        if diff_p is not None:
            print(f"  Steiger's test p-value:         p = {diff_p:.4e}")
            if diff_p < 0.05 and diff < 0:
                print(f"  → Self-confidence significantly MORE correlated with {METRIC} than other-confidence")
                print(f"    This suggests the model is introspecting on its own uncertainty,")
                print(f"    not just assessing question difficulty.")
            elif diff_p < 0.05 and diff > 0:
                print(f"  → Self-confidence significantly LESS correlated with {METRIC} than other-confidence")
                print(f"    This is unexpected - the model may be using question difficulty as a proxy.")
            else:
                print(f"  → No significant difference between self and other confidence correlations")
        else:
            print(f"  → Could not compute significance test")

    # Delegate-specific summary statistics
    if META_TASK == "delegate" and "delegation_rate" in behavioral:
        print("\n--- Delegate Task Summary ---")
        print(f"  Delegation rate:      {behavioral['delegation_rate']:.1%} ({behavioral['num_delegated']} delegated, {behavioral['num_self_answered']} self-answered)")
        if "self_answer_accuracy" in behavioral:
            print(f"  Self-answer accuracy: {behavioral['self_answer_accuracy']:.1%} ({behavioral['self_correct']}/{behavioral['num_self_answered']} correct)")
            print(f"  Teammate accuracy:    {behavioral['teammate_accuracy']:.1%} (by design)")
            print(f"  Team score:           {behavioral['team_score']:.1f} / {behavioral['num_delegated'] + behavioral['num_self_answered']} ({behavioral['team_score_normalized']:.1%})")

    print("\n--- Probe Analysis by Layer ---")
    print(f"{'Layer':<8} {'Direct→Direct':<15} {'D→M (fixed)':<15} {'D→M (orig)':<15} {'Meta→Meta':<15} {'Shuffled':<12}")
    print(f"{'':8} {'R²':<15} {'R²':<15} {'R²':<15} {'R²':<15} {'R²':<12}")
    print("-" * 110)

    for layer_idx in sorted(results.keys()):
        res = results[layer_idx]
        d2d = res["direct_to_direct"]["test_r2"]
        d2m_fixed = res["direct_to_meta_fixed"]["r2"]
        d2m_orig = res["direct_to_meta"]["r2"]
        m2m = res["meta_to_meta"]["test_r2"]
        shuf = res["shuffled_baseline"]["r2"]
        print(f"{layer_idx:<8} {d2d:<15.4f} {d2m_fixed:<15.4f} {d2m_orig:<15.4f} {m2m:<15.4f} {shuf:<12.4f}")

    print("=" * 110)

    # Summary statistics
    layers = sorted(results.keys())

    best_d2d_layer = max(layers, key=lambda l: results[l]["direct_to_direct"]["test_r2"])
    best_d2d = results[best_d2d_layer]["direct_to_direct"]["test_r2"]

    best_d2m_fixed_layer = max(layers, key=lambda l: results[l]["direct_to_meta_fixed"]["r2"])
    best_d2m_fixed = results[best_d2m_fixed_layer]["direct_to_meta_fixed"]["r2"]

    best_m2m_layer = max(layers, key=lambda l: results[l]["meta_to_meta"]["test_r2"])
    best_m2m = results[best_m2m_layer]["meta_to_meta"]["test_r2"]

    print(f"\nBest Direct→Direct:      Layer {best_d2d_layer} (R² = {best_d2d:.4f})")
    print(f"Best Direct→Meta (fixed): Layer {best_d2m_fixed_layer} (R² = {best_d2m_fixed:.4f})")
    print(f"Best Meta→Meta:          Layer {best_m2m_layer} (R² = {best_m2m:.4f})")

    # Transfer ratio using fixed D→M
    if best_d2d > 0:
        transfer_ratio = best_d2m_fixed / best_d2d
        print(f"\nTransfer ratio (best D→M fixed / best D→D): {transfer_ratio:.2%}")
        if transfer_ratio > 0.5:
            print("  → Strong evidence for introspection!")
        elif transfer_ratio > 0.25:
            print("  → Moderate evidence for introspection")
        elif transfer_ratio > 0:
            print("  → Weak evidence for introspection")
        else:
            print("  → No evidence for introspection (negative transfer)")

def plot_results(
    results: Dict,
    behavioral: Dict,
    direct_entropies: np.ndarray,
    test_idx: np.ndarray,
    output_path: str = "introspection_results.png",
    mc_answer_results: Optional[Dict] = None
):
    """
    Create visualization of results with 4 panels.
    
    Panel 1: Separate Scaler (Upper Bound).
    Panel 2: Centered Scaler (Rigorous).
    Panel 3: Pearson Correlation.
    Panel 4: NORMALIZED Timecourse (Min-Max). Compares signal emergence timing.
    """
    layers = sorted(results.keys())

    # --- Data Extraction ---
    # 1. Entropy Metrics
    ent_d2d_r2 = [results[l]["direct_to_direct"]["test_r2"] for l in layers]
    ent_d2m_separate_r2 = [results[l]["direct_to_meta_fixed"]["r2"] for l in layers] 
    ent_d2m_centered_r2 = [results[l]["direct_to_meta_centered"]["r2"] for l in layers] 
    ent_shuffled_r2 = [results[l]["shuffled_baseline"]["r2"] for l in layers]
    ent_pearson = [results[l]["direct_to_meta"]["pearson"] for l in layers]
    ent_d2d_pearson = [results[l]["direct_to_direct"]["test_pearson"] for l in layers]
    
    # 2. MC Answer Metrics
    has_mc = mc_answer_results is not None
    if has_mc:
        mc_d2d_acc = [mc_answer_results[l]["d2d_accuracy"] for l in layers]
        # Robust check for new keys
        first_layer = mc_answer_results[layers[0]]
        if "d2m_centered_accuracy" in first_layer:
            mc_d2m_sep_acc = [mc_answer_results[l]["d2m_separate_accuracy"] for l in layers]
            mc_d2m_cen_acc = [mc_answer_results[l]["d2m_centered_accuracy"] for l in layers]
        else:
            mc_d2m_sep_acc = [mc_answer_results[l]["d2m_accuracy"] for l in layers]
            mc_d2m_cen_acc = mc_d2m_sep_acc
        
        mc_chance = 0.25
    
    # --- Plotting ---
    fig, axes = plt.subplots(2, 2, figsize=(18, 12))
    plt.suptitle(f'Introspection Analysis: {get_model_short_name(BASE_MODEL_NAME)} on {DATASET_NAME} ({META_TASK})', fontsize=16)

    # Helper: Min-Max Normalization
    def normalize(data):
        arr = np.array(data)
        return (arr - arr.min()) / (arr.max() - arr.min() + 1e-9)

    # Helper for common entropy baselines
    def plot_ent_baselines(ax, metric_d2d, metric_shuffled=None):
        ax.plot(layers, metric_d2d, 'o-', label='Entropy D→D', color='tab:blue', linewidth=2)
        if metric_shuffled is not None:
            ax.plot(layers, metric_shuffled, ':', label='Entropy Shuffled', color='gray', alpha=0.6)

    # Helper for MC lines
    def plot_mc_lines(ax, d2d, d2m, label_suffix=""):
        if has_mc:
            ax.plot(layers, d2d, 'd-', label='MC Ans D→D', color='tab:green', linewidth=2)
            ax.plot(layers, d2m, 'd-', label=f'MC Ans D→M {label_suffix}', color='tab:red', linewidth=2)
            ax.axhline(y=mc_chance, color='tab:green', linestyle=':', alpha=0.4, label='Chance')

    # =========================================================================
    # Panel 1: Method 1 - Separate Scaler (Upper Bound)
    # =========================================================================
    ax1 = axes[0, 0]
    plot_ent_baselines(ax1, ent_d2d_r2, ent_shuffled_r2)
    ax1.plot(layers, ent_d2m_separate_r2, 's-', label='Entropy D→M Sep', color='tab:orange', linewidth=2)
    if has_mc:
        plot_mc_lines(ax1, mc_d2d_acc, mc_d2m_sep_acc, "Sep ")

    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax1.set_xlabel('Layer Index')
    ax1.set_ylabel('$R^2$ / Accuracy')
    ax1.set_title('Method 1: Separate Scaler (Upper Bound)\n(Absolute Performance)')
    ax1.legend(loc='lower center', bbox_to_anchor=(0.5, 0.0), ncol=2, fontsize=8, framealpha=0.95)
    ax1.grid(True, alpha=0.3)
    bottom_y = min(min(ent_shuffled_r2), min(ent_d2m_separate_r2), -0.1)
    ax1.set_ylim(bottom=max(-2.0, bottom_y - 0.1), top=1.05)


    # =========================================================================
    # Panel 2: Method 2 - Centered Scaler (Rigorous)
    # =========================================================================
    ax2 = axes[0, 1]
    plot_ent_baselines(ax2, ent_d2d_r2, ent_shuffled_r2)
    ax2.plot(layers, ent_d2m_centered_r2, 's-', label='Entropy D→M Cen', color='tab:orange', linewidth=2)
    if has_mc:
        plot_mc_lines(ax2, mc_d2d_acc, mc_d2m_cen_acc, "Cen ")

    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax2.set_xlabel('Layer Index')
    ax2.set_ylabel('$R^2$ / Accuracy')
    ax2.set_title('Method 2: Centered Scaler (Rigorous)\n(Geometry Check)')
    ax2.legend(loc='lower center', bbox_to_anchor=(0.5, 0.0), ncol=2, fontsize=8, framealpha=0.95)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(bottom=max(-2.0, bottom_y - 0.1), top=1.05)


    # =========================================================================
    # Panel 3: Method 3 - Pearson Correlation
    # =========================================================================
    ax3 = axes[1, 0]
    ax3.plot(layers, ent_d2d_pearson, 'o-', label='Entropy D→D Pearson', color='tab:blue', linewidth=2)
    ax3.plot(layers, ent_pearson, 's-', label='Entropy D→M Pearson', color='tab:orange', linewidth=2)

    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax3.set_xlabel('Layer Index')
    ax3.set_ylabel('Correlation (r)')
    ax3.set_title('Method 3: Pearson Correlation\n(Shift Invariant Signal Check)')
    ax3.legend(loc='lower right', fontsize=9)
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim(-0.1, 1.05)


    # =========================================================================
    # Panel 4: Normalized Timecourse (Min-Max Scaled)
    # This visualizes the "Pointer vs Direct Access" timing
    # =========================================================================
    ax4 = axes[1, 1]
    
    # 1. Entropy D->D (Baseline Logic)
    ax4.plot(layers, normalize(ent_d2d_r2), 'o-', label='Entropy D→D', color='tab:blue', alpha=0.4)
    
    # 2. Entropy D->M Separate (Introspection Signal)
    ax4.plot(layers, normalize(ent_d2m_separate_r2), 's-', label='Entropy D→M', color='tab:orange', linewidth=2.5)
    
    if has_mc:
        # 3. MC Answer D->D (Baseline Fact)
        ax4.plot(layers, normalize(mc_d2d_acc), 'd-', label='MC Ans D→D', color='tab:green', alpha=0.4)
        
        # 4. MC Answer D->M (Transferred Fact)
        ax4.plot(layers, normalize(mc_d2m_sep_acc), 'd-', label='MC Ans D→M', color='tab:red', linewidth=2.5)

    ax4.set_xlabel('Layer Index')
    ax4.set_ylabel('Normalized Score (0-1)')
    ax4.set_title('Signal Emergence (Min-Max Scaled)\nCheck: Do Red/Orange lines rise together?')
    ax4.legend(loc='best', fontsize=9)
    ax4.grid(True, alpha=0.3)
    ax4.set_ylim(-0.05, 1.05)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nPlot saved to {output_path}")
                    
def get_mc_prefix() -> str:
    """Get prefix for mc_entropy_probe.py output files."""
    model_short = get_model_short_name(BASE_MODEL_NAME)
    if MODEL_NAME != BASE_MODEL_NAME:
        adapter_short = get_model_short_name(MODEL_NAME)
        return str(OUTPUTS_DIR / f"{model_short}_adapter-{adapter_short}_{DATASET_NAME}_mc")
    return str(OUTPUTS_DIR / f"{model_short}_{DATASET_NAME}_mc")


def try_load_mc_data() -> Optional[Dict]:
    """
    Try to load existing MC data from mc_entropy_probe.py output.

    Returns dict with direct_activations, direct_metrics, metadata
    if files exist and config matches. Returns None otherwise.

    direct_metrics is a dict mapping metric names to numpy arrays.
    """
    mc_prefix = get_mc_prefix()
    activations_path = Path(f"{mc_prefix}_activations.npz")
    dataset_path = Path(f"{mc_prefix}_dataset.json")

    if not activations_path.exists() or not dataset_path.exists():
        return None

    print(f"\nFound existing MC data: {mc_prefix}")

    # Load and verify config
    with open(dataset_path) as f:
        dataset_data = json.load(f)

    config = dataset_data.get("config", {})
    if config.get("dataset") != DATASET_NAME:
        print(f"  Dataset mismatch: {config.get('dataset')} vs {DATASET_NAME}")
        return None
    if config.get("num_questions") != NUM_QUESTIONS:
        print(f"  Question count mismatch: {config.get('num_questions')} vs {NUM_QUESTIONS}")
        return None
    if config.get("seed") != SEED:
        print(f"  Seed mismatch: {config.get('seed')} vs {SEED}")
        return None

    # Load activations
    print(f"  Loading activations from {activations_path}...")
    acts_data = np.load(activations_path)
    direct_activations = {
        int(k.split("_")[1]): acts_data[k]
        for k in acts_data.files if k.startswith("layer_")
    }

    # Load all metrics (new format stores: entropy, top_prob, margin, logit_gap, top_logit)
    # Fall back to "entropies" key for backward compatibility with old files
    direct_metrics = {}
    metric_keys = ["entropy", "top_prob", "margin", "logit_gap", "top_logit"]
    for key in metric_keys:
        if key in acts_data.files:
            direct_metrics[key] = acts_data[key]
    # Backward compatibility: old files have "entropies" key
    if not direct_metrics and "entropies" in acts_data.files:
        direct_metrics["entropy"] = acts_data["entropies"]

    # Get metadata (includes questions, probs, etc.)
    metadata = dataset_data.get("data", [])

    print(f"  Loaded {len(direct_activations)} layers, {len(direct_metrics.get('entropy', []))} questions")
    print(f"  Metrics available: {list(direct_metrics.keys())}")
    print(f"  Reusing direct activations from mc_entropy_probe.py!")

    return {
        "direct_activations": direct_activations,
        "direct_metrics": direct_metrics,
        "metadata": metadata,
    }


# ============================================================================
# MAIN
# ============================================================================

def run_single_experiment(
    dataset_name: str,
    meta_task: str,
    model,
    tokenizer,
    num_layers: int,
    metric: str,
    batch_size: int,
    collect_other_activations: bool = False
):
    """Run a single introspection experiment for one dataset/task combination."""
    global DATASET_NAME, META_TASK, NUM_QUESTIONS, METRIC

    # Update global variables for this run
    DATASET_NAME = dataset_name
    META_TASK = meta_task
    METRIC = metric
    NUM_QUESTIONS = NUM_QUESTIONS_BY_DATASET.get(dataset_name, NUM_QUESTIONS_DEFAULT)

    print("\n" + "=" * 80)
    print(f"Running: {dataset_name} / {meta_task} / {metric}")
    print("=" * 80)

    # Print delegate parameters if using delegate task
    if meta_task == "delegate":
        print("\n--- Delegate Task Parameters ---")
        print("  (Matching delegate_game_from_capabilities.py)")
        print("  decision_only: True")
        print("  alternate_decision_mapping: True")
        print("  use_phase1_summary: True")
        print("  use_phase1_history: False")
        print("  use_examples: True")
        print("  teammate_accuracy: 50%")
        print("  Options: 1/2 (alternating mapping per trial)")
        print("")

    # Check for existing MC data first
    mc_data = try_load_mc_data()

    # Load questions
    print(f"\nLoading {NUM_QUESTIONS} questions from {dataset_name}...")
    questions = load_questions(dataset_name, NUM_QUESTIONS)
    # Re-seed immediately before shuffle to match capabilities_test.py exactly
    random.seed(SEED)
    random.shuffle(questions)
    print(f"Loaded {len(questions)} questions")

    # Determine whether to use chat template
    use_chat_template = has_chat_template(tokenizer) and not is_base_model(BASE_MODEL_NAME)
    print(f"Using chat template: {use_chat_template}")

    # Collect paired data (direct and meta for each question)
    # If we have existing MC data, only run meta prompts
    if mc_data is not None:
        data = collect_meta_only(questions, model, tokenizer, num_layers, use_chat_template, mc_data, batch_size=batch_size)
    else:
        data = collect_paired_data(questions, model, tokenizer, num_layers, use_chat_template, batch_size=batch_size)

    # Generate output prefixes
    # Base prefix for shared files (activations, paired data)
    base_prefix = get_output_prefix()
    # Metric-specific prefix for probe results (task-dependent)
    metric_prefix = get_output_prefix(metric)
    # Directions prefix (task-independent - directions are the same for confidence/delegate)
    directions_prefix = get_directions_prefix(metric)
    print(f"Base output prefix: {base_prefix}")
    print(f"Metric output prefix: {metric_prefix}")
    print(f"Directions prefix: {directions_prefix}")

    # Get the selected metric's values
    direct_target = data["direct_metrics"][METRIC]

    # Save activations with ALL metrics (so we can retrain with different metrics later)
    print("\nSaving activations...")
    np.savez_compressed(
        f"{base_prefix}_direct_activations.npz",
        **{f"layer_{i}": acts for i, acts in data["direct_activations"].items()},
        **data["direct_metrics"]  # Save all metrics
    )
    np.savez_compressed(
        f"{base_prefix}_meta_activations.npz",
        **{f"layer_{i}": acts for i, acts in data["meta_activations"].items()},
        entropy=data["meta_entropies"]  # Meta always uses entropy
    )
    print(f"Saved activations to {base_prefix}_*_activations.npz")

    # Generate example prompts for verification (first 2 questions)
    example_prompts = []
    for i in range(min(2, len(questions))):
        q = questions[i]
        direct_prompt, direct_options = format_direct_prompt(q, tokenizer, use_chat_template)
        if META_TASK == "delegate":
            meta_prompt, meta_options_list, mapping = format_delegate_prompt(q, tokenizer, use_chat_template, trial_index=i)
        else:
            meta_prompt, meta_options_list = format_meta_prompt(q, tokenizer, use_chat_template)
            mapping = None
        example_prompts.append({
            "question_index": i,
            "question_text": q.get("question", ""),
            "direct_prompt": direct_prompt,
            "direct_options": direct_options,
            "meta_prompt": meta_prompt,
            "meta_options": meta_options_list,
            "meta_mapping": mapping,
        })

    # Collect other-confidence control data (only for confidence task)
    # Do this before saving paired_data so we can include it
    other_data = None
    if META_TASK == "confidence":
        print("\n" + "=" * 60)
        print("Running OTHER-CONFIDENCE control task...")
        print("(Asks model to estimate what % of college-educated people would know the answer)")
        print("=" * 60)

        other_data = collect_other_confidence(
            questions, model, tokenizer, num_layers, use_chat_template,
            batch_size=batch_size, collect_activations=collect_other_activations
        )

        # Add example prompts for other-confidence
        for i in range(min(2, len(questions))):
            q = questions[i]
            other_prompt, _ = format_other_confidence_prompt(q, tokenizer, use_chat_template)
            if i < len(example_prompts):
                example_prompts[i]["other_confidence_prompt"] = other_prompt

    # Save paired data (for reproducibility and further analysis)
    paired_data = {
        "direct_metrics": {k: v.tolist() for k, v in data["direct_metrics"].items()},
        "direct_probs": data["direct_probs"],
        "direct_logits": data.get("direct_logits", []),
        "meta_entropies": data["meta_entropies"].tolist(),
        "meta_probs": data["meta_probs"],
        "meta_responses": data["meta_responses"],
        "meta_mappings": data.get("meta_mappings"),  # Store mappings for delegate task
        "questions": [
            {
                "id": q.get("id", f"q_{i}"),
                "question": q.get("question", ""),
                "correct_answer": q.get("correct_answer", ""),
                "options": q.get("options", {})
            }
            for i, q in enumerate(data["questions"])
        ],
        "example_prompts": example_prompts,  # For verification of prompt formatting
        "config": {
            "model_name": MODEL_NAME,
            "base_model_name": BASE_MODEL_NAME,
            "dataset_name": DATASET_NAME,
            "num_questions": NUM_QUESTIONS,
            "seed": SEED,
            "meta_task": META_TASK,
            "metric": METRIC,
            # Delegate task parameters (matches delegate_game_from_capabilities.py)
            "delegate_params": {
                "decision_only": True,
                "alternate_decision_mapping": True,
                "use_phase1_summary": True,
                "use_phase1_history": False,
                "use_examples": True,
                "teammate_accuracy": 0.5,
            } if META_TASK == "delegate" else None,
        }
    }

    # Add other-confidence data if collected
    if other_data is not None:
        paired_data["other_confidence"] = {
            "probs": other_data["other_probs"],
            "responses": other_data["other_responses"],
            "signals": other_data["other_signals"].tolist(),
        }

    with open(f"{base_prefix}_paired_data.json", "w") as f:
        json.dump(paired_data, f, indent=2)
    print(f"Saved paired data to {base_prefix}_paired_data.json")

    # Run introspection analysis with selected metric
    print(f"\nRunning introspection analysis with metric: {METRIC}")
    results, test_idx, directions, entropy_probe_components = run_introspection_analysis(
        data["direct_activations"],
        data["meta_activations"],
        direct_target,  # Use selected metric
        extract_directions=True
    )

    # Save directions for steering/ablation experiments (task-independent filename)
    # Directions are task-independent because they predict metrics from direct task activations
    if directions is not None:
        directions_data = {
            f"layer_{layer_idx}": direction
            for layer_idx, direction in directions.items()
        }
        directions_data["_metadata_metric"] = np.array(METRIC)
        directions_data["_metadata_dataset"] = np.array(DATASET_NAME)
        directions_data["_metadata_model"] = np.array(BASE_MODEL_NAME)
        np.savez_compressed(
            f"{directions_prefix}_directions.npz",
            **directions_data
        )
        print(f"Saved {METRIC} directions to {directions_prefix}_directions.npz")

    # Compute model's predicted MC answer (argmax over A/B/C/D probabilities)
    # This is the answer the model would give on the direct task
    direct_probs_array = np.array([p if p else [0.25, 0.25, 0.25, 0.25] for p in data["direct_probs"]])
    model_predicted_answer = np.argmax(direct_probs_array, axis=1)  # 0=A, 1=B, 2=C, 3=D
    print(f"\nModel predicted answers: {len(model_predicted_answer)} questions")
    print(f"  Answer distribution: A={np.sum(model_predicted_answer==0)}, B={np.sum(model_predicted_answer==1)}, "
          f"C={np.sum(model_predicted_answer==2)}, D={np.sum(model_predicted_answer==3)}")

    # Get train/test indices from introspection analysis (need to recompute since run_introspection_analysis
    # returns test_idx but not train_idx)
    n_questions = len(direct_target)
    indices = np.arange(n_questions)
    train_idx, _ = train_test_split(indices, train_size=TRAIN_SPLIT, random_state=SEED)

    # Run MC answer probe analysis
    mc_answer_results, mc_probe_components = run_mc_answer_analysis(
        data["direct_activations"],
        data["meta_activations"],
        model_predicted_answer,
        train_idx,
        test_idx
    )

    # Print MC answer probe summary
    best_mc_d2d_layer = max(mc_answer_results.keys(), key=lambda l: mc_answer_results[l]["d2d_accuracy"])
    best_mc_d2m_layer = max(mc_answer_results.keys(), key=lambda l: mc_answer_results[l]["d2m_accuracy"])
    print(f"\nMC Answer Probe Results:")
    print(f"  Best D→D: Layer {best_mc_d2d_layer} (acc={mc_answer_results[best_mc_d2d_layer]['d2d_accuracy']:.3f})")
    print(f"  Best D→M: Layer {best_mc_d2m_layer} (acc={mc_answer_results[best_mc_d2m_layer]['d2m_accuracy']:.3f})")
    print(f"  Chance: 0.250 (4-class)")

    # Behavioral analysis (uses selected METRIC for correlation with stated confidence)
    behavioral = analyze_behavioral_introspection(
        data["meta_responses"],
        data["direct_metrics"][METRIC],  # Use the selected metric for behavioral correlation
        test_idx,
        data["meta_probs"],
        data.get("meta_mappings"),
        data["direct_probs"],
        data["questions"]
    )

    # Other-confidence control analysis (only for confidence task)
    # other_data was collected earlier, now we analyze it
    other_confidence_analysis = None
    if META_TASK == "confidence" and other_data is not None:
        # Compute self-confidence signals for comparison
        self_confidence = np.array([
            get_stated_confidence_signal(np.array(p)) if p else 0.5
            for p in data["meta_probs"]
        ])

        other_confidence_analysis = analyze_other_confidence_control(
            other_data["other_signals"],
            self_confidence,
            data["direct_metrics"][METRIC],
            test_idx
        )

    # Other-confidence probe analysis (only when activations were collected)
    # Uses trained probe components from main analysis - only computes D→M(Other)
    other_probe_results = None
    if META_TASK == "confidence" and other_data is not None and "other_activations" in other_data:
        print("\n" + "=" * 60)
        print("Running OTHER-CONFIDENCE TRANSFER ANALYSIS...")
        print("(Testing whether direct-trained probes transfer to other-confidence)")
        print("=" * 60)

        other_probe_results = apply_probes_to_other(
            other_data["other_activations"],
            entropy_probe_components,
            mc_probe_components,
            direct_target,
            model_predicted_answer,
            test_idx
        )

        # Print summary
        best_entropy_layer = max(other_probe_results.keys(), key=lambda l: other_probe_results[l]["d2m_other_entropy_r2"])
        best_mc_layer = max(other_probe_results.keys(), key=lambda l: other_probe_results[l]["d2m_other_mc_accuracy"])
        print(f"\nDirect Probe Transfer to Other-Confidence:")
        print(f"  Best Entropy D→M(Other): Layer {best_entropy_layer} (R²={other_probe_results[best_entropy_layer]['d2m_other_entropy_r2']:.3f})")
        print(f"  Best MC D→M(Other): Layer {best_mc_layer} (acc={other_probe_results[best_mc_layer]['d2m_other_mc_accuracy']:.3f})")

    # Calibration split analysis: split test set into calibrated vs miscalibrated trials
    # Uses stated_confidence from behavioral analysis
    stated_confidence = np.array(behavioral["stated_confidence"])
    test_confidence = stated_confidence[test_idx]
    test_metric = direct_target[test_idx]

    # Determine if the current metric indicates uncertainty (high = uncertain) or confidence (high = confident)
    metric_is_uncertainty = METRIC in UNCERTAINTY_METRICS
    calibrated_mask, miscalibrated_mask = compute_calibration_masks(
        test_confidence, test_metric, metric_is_uncertainty=metric_is_uncertainty
    )

    print(f"\nCalibration split (median) for {METRIC}:")
    if metric_is_uncertainty:
        print(f"  Calibrated: {calibrated_mask.sum()} trials (high conf + low {METRIC}, or low conf + high {METRIC})")
        print(f"  Miscalibrated: {miscalibrated_mask.sum()} trials (high conf + high {METRIC}, or low conf + low {METRIC})")
    else:
        print(f"  Calibrated: {calibrated_mask.sum()} trials (high conf + high {METRIC}, or low conf + low {METRIC})")
        print(f"  Miscalibrated: {miscalibrated_mask.sum()} trials (high conf + low {METRIC}, or low conf + high {METRIC})")

    # Split probe results by calibration
    calibration_split = split_results_by_calibration(
        results, direct_target[test_idx], calibrated_mask, miscalibrated_mask
    )

    # Print summary of calibration split
    if calibrated_mask.sum() > 1 and miscalibrated_mask.sum() > 1:
        best_cal_d2m_layer = max(calibration_split.keys(),
                                  key=lambda l: calibration_split[l]["calibrated"]["d2m_r2"])
        best_mis_d2m_layer = max(calibration_split.keys(),
                                  key=lambda l: calibration_split[l]["miscalibrated"]["d2m_r2"])
        print(f"\n  Calibrated best D→M: Layer {best_cal_d2m_layer} "
              f"(R²={calibration_split[best_cal_d2m_layer]['calibrated']['d2m_r2']:.3f})")
        print(f"  Miscalibrated best D→M: Layer {best_mis_d2m_layer} "
              f"(R²={calibration_split[best_mis_d2m_layer]['miscalibrated']['d2m_r2']:.3f})")

    # Plot calibration split
    plot_calibration_split(
        calibration_split,
        n_calibrated=int(calibrated_mask.sum()),
        n_miscalibrated=int(miscalibrated_mask.sum()),
        output_path=f"{metric_prefix}_calibration_split.png"
    )

    # Plot other-confidence probe comparison (only when available)
    if other_probe_results is not None:
        plot_other_confidence_comparison(
            results,
            mc_answer_results,
            other_probe_results,
            output_path=f"{metric_prefix}_other_confidence_transfer.png"
        )

    # Save results (metric-specific filename)
    results_to_save = {
        "config": {
            "metric": METRIC,
            "meta_task": META_TASK,
            "model": BASE_MODEL_NAME,
            "dataset": DATASET_NAME,
        },
        "metric_stats": {
            "mean": float(direct_target.mean()),
            "std": float(direct_target.std()),
            "min": float(direct_target.min()),
            "max": float(direct_target.max()),
        },
        "probe_results": {
            str(layer_idx): {
                k: (v.tolist() if isinstance(v, np.ndarray) else v)
                for k, v in layer_results.items()
                if not isinstance(v, dict) or k in ["direct_to_direct", "direct_to_meta", "direct_to_meta_fixed", "shuffled_baseline", "meta_to_meta"]
            }
            for layer_idx, layer_results in results.items()
        },
        "behavioral": behavioral,
        "other_confidence_analysis": other_confidence_analysis,  # None if not confidence task
        "test_indices": test_idx.tolist(),
        "mc_answer_probe": {
            str(layer_idx): {
                "d2d_accuracy": layer_results["d2d_accuracy"],
                "d2m_accuracy": layer_results["d2m_accuracy"],
                "shuffled_accuracy": layer_results["shuffled_accuracy"],
            }
            for layer_idx, layer_results in mc_answer_results.items()
        },
        "model_predicted_answer": model_predicted_answer.tolist(),
        "calibration_split": {
            str(layer_idx): layer_data
            for layer_idx, layer_data in calibration_split.items()
        },
        "calibration_counts": {
            "calibrated": int(calibrated_mask.sum()),
            "miscalibrated": int(miscalibrated_mask.sum()),
        },
    }

    # Add other-confidence probe results if available
    if other_probe_results is not None:
        results_to_save["other_confidence_transfer"] = {
            str(layer_idx): {
                "d2m_other_entropy_r2": layer_data["d2m_other_entropy_r2"],
                "d2m_other_mc_accuracy": layer_data["d2m_other_mc_accuracy"],
            }
            for layer_idx, layer_data in other_probe_results.items()
        }

    # Properly serialize nested dicts
    for layer_idx in results_to_save["probe_results"]:
        for key in ["direct_to_direct", "direct_to_meta", "direct_to_meta_fixed", "shuffled_baseline", "meta_to_meta"]:
            if key in results_to_save["probe_results"][layer_idx]:
                inner = results_to_save["probe_results"][layer_idx][key]
                for k, v in inner.items():
                    if isinstance(v, np.ndarray):
                        inner[k] = v.tolist()

    with open(f"{metric_prefix}_results.json", "w") as f:
        json.dump(results_to_save, f, indent=2)
    print(f"Saved results to {metric_prefix}_results.json")

    # Print and plot results
    print_results(results, behavioral, other_confidence_analysis)
    plot_results(
        results, behavioral,
        direct_target, test_idx,
        output_path=f"{metric_prefix}_results.png",
        mc_answer_results=mc_answer_results
    )

    print(f"\n✓ Introspection experiment complete! ({dataset_name} / {meta_task} / {metric})")


def main():
    parser = argparse.ArgumentParser(description="Run introspection experiment")
    parser.add_argument("--metric", type=str, default=METRIC, choices=AVAILABLE_METRICS,
                        help=f"Uncertainty metric to probe (default: {METRIC})")
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE,
                        help=f"Batch size for forward passes (default {BATCH_SIZE})")
    parser.add_argument("--load-in-4bit", action="store_true", default=LOAD_IN_4BIT,
                        help=f"Load model in 4-bit quantization (default: {LOAD_IN_4BIT})")
    parser.add_argument("--load-in-8bit", action="store_true", default=LOAD_IN_8BIT,
                        help=f"Load model in 8-bit quantization (default: {LOAD_IN_8BIT})")
    parser.add_argument("--collect-other-activations", action="store_true", default=False,
                        help="Collect activations during other-confidence task for probe analysis")
    args = parser.parse_args()

    print(f"Device: {DEVICE}")
    print(f"Metric: {args.metric}")
    print(f"Datasets to process: {DATASETS}")
    print(f"Meta-tasks to process: {META_TASKS}")
    print(f"Total combinations: {len(DATASETS) * len(META_TASKS)}")

    # Load model and tokenizer ONCE using shared utility
    print("\nLoading model (this will be shared across all experiments)...")
    model, tokenizer, num_layers = load_model_and_tokenizer(
        BASE_MODEL_NAME,
        adapter_path=MODEL_NAME if MODEL_NAME != BASE_MODEL_NAME else None,
        load_in_4bit=args.load_in_4bit,
        load_in_8bit=args.load_in_8bit
    )

    # Run all dataset/task combinations
    for dataset_name in DATASETS:
        for meta_task in META_TASKS:
            run_single_experiment(
                dataset_name=dataset_name,
                meta_task=meta_task,
                model=model,
                tokenizer=tokenizer,
                num_layers=num_layers,
                metric=args.metric,
                batch_size=args.batch_size,
                collect_other_activations=args.collect_other_activations
            )

    print("\n" + "=" * 80)
    print("✓ All experiments complete!")
    print("=" * 80)


if __name__ == "__main__":
    main()
