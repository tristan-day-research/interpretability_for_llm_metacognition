"""
Run uncertainty prediction experiment on diverse text.

This script:
1. Loads the stratified dataset from build_nexttoken_dataset.py
2. Extracts activations from all layers of the model
3. Computes multiple uncertainty metrics (all saved, one or all probed per run):

   Prob-based (nonlinear - may be harder for linear probes):
   - entropy: Shannon entropy -sum(p * log(p))
   - top_prob: P(argmax) - probability of most likely token
   - margin: P(top) - P(second) - prob gap between top two

   Logit-based (linear - better aligned with linear probes):
   - logit_gap: z(top) - z(second) - logit gap between top two
   - top_logit: z(top) - mean(z) - centered top logit

4. Trains linear probes to predict the selected metric(s) from each layer
5. Saves directions: {prefix}_{metric}_directions.npz

Supports resuming from checkpoint if interrupted during extraction.

Usage:
    python nexttoken_entropy_probe.py                     # Probe entropy (default)
    python nexttoken_entropy_probe.py --metric logit_gap  # Probe logit_gap
    python nexttoken_entropy_probe.py --all-metrics       # Probe all metrics
    python nexttoken_entropy_probe.py --plot-only         # Load saved activations, retrain probes
"""

import argparse
import torch
import numpy as np
import json
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Tuple, Optional
import matplotlib.pyplot as plt
from joblib import Parallel, delayed

from core import (
    DEVICE,
    load_model_and_tokenizer,
    get_model_short_name,
    LinearProbe,
    compute_entropy_from_probs,
)

# Configuration
BASE_MODEL_NAME = "meta-llama/Llama-3.3-70B-Instruct"
MODEL_NAME = BASE_MODEL_NAME  # Set to adapter path for fine-tuned model
DATASET_PATH = None  # Auto-detect based on model name, or set explicitly
MAX_PROMPT_LENGTH = 500
SEED = 42
CHECKPOINT_INTERVAL = 200  # Save checkpoint every N prompts

# Output directory
OUTPUTS_DIR = Path("outputs")
OUTPUTS_DIR.mkdir(exist_ok=True)

# Probe training config
TRAIN_SPLIT = 0.8
PROBE_ALPHA = 1000.0
USE_PCA = True
PCA_COMPONENTS = 100
N_BOOTSTRAP = 100  # Number of bootstrap iterations for confidence intervals

# Available uncertainty metrics:
# Prob-based (nonlinear targets - may be harder for linear probes):
#   entropy   - Shannon entropy -sum(p * log(p))
#   top_prob  - P(argmax) - probability of most likely answer
#   margin    - P(top) - P(second) - prob gap between top two
# Logit-based (linear targets - better aligned with linear probes):
#   logit_gap - z(top) - z(second) - logit gap between top two
#   top_logit - z(top) - mean(z) - centered top logit
AVAILABLE_METRICS = ["entropy", "top_prob", "margin", "logit_gap", "top_logit"]
METRIC = "entropy"  # Which metric to probe (set via --metric flag)

np.random.seed(SEED)
torch.manual_seed(SEED)


def compute_uncertainty_metrics(probs: np.ndarray, logits: np.ndarray = None) -> Dict[str, float]:
    """
    Compute multiple uncertainty metrics from probability and logit distributions.

    Args:
        probs: Probability distribution over next tokens (after softmax)
        logits: Raw logits (before softmax). If None, logit-based metrics
                will be computed from log(probs) as an approximation.

    Returns:
        Dict with keys: entropy, top_prob, margin, logit_gap, top_logit
    """
    # === Prob-based metrics (nonlinear) ===

    # Entropy: -sum(p * log(p))
    entropy = compute_entropy_from_probs(probs)

    # Top probability: P(argmax)
    top_prob = float(np.max(probs))

    # Margin: P(top) - P(second)
    sorted_probs = np.sort(probs)[::-1]  # Descending
    margin = float(sorted_probs[0] - sorted_probs[1]) if len(sorted_probs) > 1 else float(sorted_probs[0])

    # === Logit-based metrics (linear - better for linear probes) ===

    # If logits not provided, approximate from log-probs
    # (This loses the constant offset but preserves gaps)
    if logits is None:
        logits = np.log(probs + 1e-10)

    # Sort logits descending
    sorted_logits = np.sort(logits)[::-1]

    # Logit gap: z(top) - z(second)
    # This is the cleanest linear target - invariant to temperature/scale shifts
    logit_gap = float(sorted_logits[0] - sorted_logits[1]) if len(sorted_logits) > 1 else float(sorted_logits[0])

    # Top logit (centered): z(top) - mean(z)
    # Subtracting mean makes it invariant to adding a constant to all logits
    top_logit = float(sorted_logits[0] - np.mean(logits))

    return {
        "entropy": entropy,
        "top_prob": top_prob,
        "margin": margin,
        "logit_gap": logit_gap,
        "top_logit": top_logit,
    }


def find_dataset_path() -> Path:
    """Find the dataset file, auto-detecting based on model name."""
    if DATASET_PATH:
        return Path(DATASET_PATH)

    # Try model-specific path in outputs directory first
    model_short = get_model_short_name(BASE_MODEL_NAME)
    model_specific = OUTPUTS_DIR / f"{model_short}_nexttoken_entropy_dataset.json"
    if model_specific.exists():
        return model_specific

    # Fall back to generic name in outputs
    generic = OUTPUTS_DIR / "entropy_dataset.json"
    if generic.exists():
        return generic

    raise FileNotFoundError(
        f"Could not find dataset. Tried: {model_specific}, {generic}. "
        "Run build_nexttoken_dataset.py first."
    )


def load_dataset(path: Path) -> Tuple[List[Dict], Optional[Dict]]:
    """
    Load the entropy dataset.

    Returns (data, config) where config may be None for old-format files.
    """
    print(f"Loading dataset from {path}...")
    with open(path) as f:
        raw = json.load(f)

    # Handle both old format (list) and new format (dict with config)
    if isinstance(raw, dict) and "data" in raw:
        data = raw["data"]
        config = raw.get("config")
    else:
        data = raw
        config = None

    print(f"Loaded {len(data)} prompts")
    if config:
        print(f"  Dataset config: {config}")

    return data, config


def extract_all_activations_and_metrics(
    dataset: List[Dict],
    model,
    tokenizer,
    num_layers: int,
    checkpoint_path: Path
) -> Tuple[Dict[int, np.ndarray], Dict[str, np.ndarray]]:
    """
    Extract activations from all layers for all prompts and compute uncertainty metrics.
    Supports resuming from checkpoint.

    Returns:
        activations: Dict mapping layer_idx -> array of shape (num_samples, hidden_dim)
        metrics: Dict mapping metric_name -> array of shape (num_samples,)
    """
    # Check for existing checkpoint
    if checkpoint_path.exists():
        print(f"Loading checkpoint from {checkpoint_path}...")
        checkpoint = np.load(checkpoint_path, allow_pickle=True)

        # Handle both old and new checkpoint formats
        if "processed_count" in checkpoint.files:
            start_idx = int(checkpoint["processed_count"])
            all_layer_activations = {
                int(k.split("_")[1]): list(checkpoint[k])
                for k in checkpoint.files if k.startswith("layer_")
            }
            # Load all metrics from checkpoint
            all_metrics = {metric: [] for metric in AVAILABLE_METRICS}
            for metric in AVAILABLE_METRICS:
                if metric in checkpoint.files:
                    all_metrics[metric] = list(checkpoint[metric])
                elif metric == "entropy" and "entropies" in checkpoint.files:
                    # Backward compatibility
                    all_metrics["entropy"] = list(checkpoint["entropies"])
            print(f"Resuming from prompt {start_idx}/{len(dataset)}")
        else:
            # Old checkpoint format without processed_count - start fresh
            print("Warning: Old checkpoint format detected, starting fresh extraction")
            checkpoint_path.unlink()  # Remove incompatible checkpoint
            start_idx = 0
            all_layer_activations = {i: [] for i in range(num_layers)}
            all_metrics = {metric: [] for metric in AVAILABLE_METRICS}
    else:
        start_idx = 0
        all_layer_activations = {i: [] for i in range(num_layers)}
        all_metrics = {metric: [] for metric in AVAILABLE_METRICS}

    print(f"Extracting activations from {num_layers} layers...")
    model.eval()

    # Set up hooks for activation extraction
    activations_cache = {}
    hooks = []

    def make_hook(layer_idx):
        def hook(module, input, output):
            if isinstance(output, tuple):
                hidden_states = output[0]
            else:
                hidden_states = output
            activations_cache[layer_idx] = hidden_states.detach()
        return hook

    # Get layers
    if hasattr(model, 'get_base_model'):
        base = model.get_base_model()
        layers = base.model.layers
    else:
        layers = model.model.layers

    # Register hooks
    for i, layer in enumerate(layers):
        handle = layer.register_forward_hook(make_hook(i))
        hooks.append(handle)

    try:
        for i in tqdm(range(start_idx, len(dataset))):
            item = dataset[i]
            text = item["text"]

            # Tokenize
            inputs = tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=MAX_PROMPT_LENGTH
            )
            input_ids = inputs["input_ids"].to(DEVICE)
            attention_mask = inputs["attention_mask"].to(DEVICE)

            # Clear cache
            activations_cache.clear()

            # Forward pass to get logits AND activations
            with torch.no_grad():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False)

            # Get logits at last position (for next-token prediction)
            final_logits = outputs.logits[0, -1, :].cpu().numpy()
            probs = torch.softmax(outputs.logits[0, -1, :], dim=-1).cpu().numpy()

            # Compute all uncertainty metrics
            item_metrics = compute_uncertainty_metrics(probs, final_logits)

            # Store metrics
            for metric_name, metric_value in item_metrics.items():
                all_metrics[metric_name].append(metric_value)

            # Extract activations at last token
            for layer_idx, acts in activations_cache.items():
                all_layer_activations[layer_idx].append(acts[0, -1, :].cpu().numpy())

            # Checkpoint
            if (i + 1) % CHECKPOINT_INTERVAL == 0:
                save_checkpoint_with_metrics(
                    all_layer_activations, all_metrics, i + 1, checkpoint_path
                )

            # Clear memory
            del inputs, input_ids, attention_mask, outputs
            if (i + 1) % 100 == 0:
                torch.cuda.empty_cache()

    finally:
        # Remove hooks
        for handle in hooks:
            handle.remove()

    # Convert to numpy arrays
    activations = {
        layer_idx: np.array(acts)
        for layer_idx, acts in all_layer_activations.items()
    }
    metrics = {
        metric_name: np.array(values)
        for metric_name, values in all_metrics.items()
    }

    print(f"Extracted activations shape (per layer): {activations[0].shape}")
    print(f"\nUncertainty metrics:")
    for metric_name, values in metrics.items():
        print(f"  {metric_name}: range=[{values.min():.3f}, {values.max():.3f}], "
              f"mean={values.mean():.3f}, std={values.std():.3f}")

    # Clean up checkpoint
    if checkpoint_path.exists():
        checkpoint_path.unlink()
        print("Removed checkpoint file")

    return activations, metrics


def save_checkpoint_with_metrics(
    layer_activations: Dict[int, List],
    metrics: Dict[str, List[float]],
    processed_count: int,
    checkpoint_path: Path
):
    """Save extraction checkpoint with all metrics."""
    save_dict = {
        f"layer_{i}": np.array(acts)
        for i, acts in layer_activations.items()
    }
    # Save all metrics
    for metric_name, values in metrics.items():
        save_dict[metric_name] = np.array(values)
    save_dict["processed_count"] = np.array(processed_count)

    np.savez_compressed(checkpoint_path, **save_dict)
    print(f"  Checkpoint saved: {processed_count} prompts")


def _train_probe_for_layer(
    layer_idx: int,
    X: np.ndarray,
    targets: np.ndarray,
    n_bootstrap: int,
    train_split: float,
    seed: int,
    use_pca: bool,
    pca_components: int,
    alpha: float
) -> Tuple[int, Dict, np.ndarray]:
    """Train probe for a single layer with bootstrap. Used for parallel execution.

    Returns (layer_idx, results_dict, direction_vector).
    Direction is extracted from a final probe trained on full training set.
    """
    rng = np.random.RandomState(seed + layer_idx)  # Reproducible per-layer
    n = len(targets)

    test_r2s = []
    test_maes = []

    # Bootstrap for confidence intervals
    for _ in range(n_bootstrap):
        # Random split
        indices = np.arange(n)
        rng.shuffle(indices)
        split_idx = int(n * train_split)
        train_idx = indices[:split_idx]
        test_idx = indices[split_idx:]

        X_train = X[train_idx]
        X_test = X[test_idx]
        y_train = targets[train_idx]
        y_test = targets[test_idx]

        # Train probe
        probe = LinearProbe(
            alpha=alpha,
            use_pca=use_pca,
            pca_components=pca_components
        )
        probe.fit(X_train, y_train)

        # Evaluate
        test_eval = probe.evaluate(X_test, y_test)
        test_r2s.append(test_eval["r2"])
        test_maes.append(test_eval["mae"])

    # Train final probe on canonical split for direction extraction
    rng_final = np.random.RandomState(seed)  # Same seed across layers for consistent split
    indices = np.arange(n)
    rng_final.shuffle(indices)
    split_idx = int(n * train_split)
    train_idx = indices[:split_idx]

    final_probe = LinearProbe(
        alpha=alpha,
        use_pca=use_pca,
        pca_components=pca_components
    )
    final_probe.fit(X[train_idx], targets[train_idx])
    direction = final_probe.get_direction()  # Always in original space

    return layer_idx, {
        "test_r2_mean": float(np.mean(test_r2s)),
        "test_r2_std": float(np.std(test_r2s)),
        "test_mae_mean": float(np.mean(test_maes)),
        "test_mae_std": float(np.std(test_maes)),
    }, direction


def run_all_probes(
    activations: Dict[int, np.ndarray],
    metrics: Dict[str, np.ndarray],
    n_jobs: int = -1,
    use_pca: bool = USE_PCA,
    pca_components: int = PCA_COMPONENTS,
    alpha: float = PROBE_ALPHA
) -> Tuple[Dict[str, Dict[int, Dict]], Dict[str, Dict[int, np.ndarray]]]:
    """Train probes for all layers and all metrics with bootstrap confidence intervals.

    Returns:
        all_results: Dict mapping metric_name -> {layer_idx -> result dict with R², MAE stats}
        all_directions: Dict mapping metric_name -> {layer_idx -> normalized direction vector}
    """
    pca_str = f"PCA={pca_components}" if use_pca else "no PCA"
    layer_indices = sorted(activations.keys())

    all_results = {}
    all_directions = {}

    for metric_name, targets in metrics.items():
        print(f"\nTraining probes for metric '{metric_name}' across {len(activations)} layers "
              f"({N_BOOTSTRAP} bootstrap iterations, {pca_str})...")

        # Run in parallel across layers
        results_list = Parallel(n_jobs=n_jobs, verbose=10)(
            delayed(_train_probe_for_layer)(
                layer_idx,
                activations[layer_idx],
                targets,
                N_BOOTSTRAP,
                TRAIN_SPLIT,
                SEED,
                use_pca,
                pca_components,
                alpha
            )
            for layer_idx in layer_indices
        )

        # Convert list of (layer_idx, result, direction) tuples to dicts
        all_results[metric_name] = {layer_idx: result for layer_idx, result, _ in results_list}
        all_directions[metric_name] = {layer_idx: direction for layer_idx, _, direction in results_list}

    return all_results, all_directions


def print_results(all_results: Dict[str, Dict[int, Dict]]):
    """Print summary of results for all metrics."""
    for metric_name, results in all_results.items():
        print("\n" + "="*80)
        print(f"RESULTS SUMMARY: {metric_name.upper()}")
        print("="*80)
        print(f"{'Layer':<8} {'Test R²':<20} {'Test MAE':<20}")
        print("-"*80)

        for layer_idx in sorted(results.keys()):
            res = results[layer_idx]
            r2_str = f"{res['test_r2_mean']:.4f} ± {res['test_r2_std']:.4f}"
            mae_str = f"{res['test_mae_mean']:.4f} ± {res['test_mae_std']:.4f}"
            print(f"{layer_idx:<8} {r2_str:<20} {mae_str:<20}")

        print("="*80)

        # Find best layer
        best_layer = max(results.keys(), key=lambda l: results[l]["test_r2_mean"])
        best_r2 = results[best_layer]["test_r2_mean"]
        best_std = results[best_layer]["test_r2_std"]
        print(f"Best layer: {best_layer} (Test R² = {best_r2:.4f} ± {best_std:.4f})")

    # Print comparison across metrics
    print("\n" + "="*80)
    print("METRIC COMPARISON (Best R² per metric)")
    print("="*80)
    for metric_name, results in all_results.items():
        best_layer = max(results.keys(), key=lambda l: results[l]["test_r2_mean"])
        best_r2 = results[best_layer]["test_r2_mean"]
        print(f"  {metric_name:<12}: layer {best_layer:<3} R² = {best_r2:.4f}")


def plot_results(all_results: Dict[str, Dict[int, Dict]], output_path: Path):
    """Plot R² across layers for all metrics with confidence intervals."""
    metric_names = list(all_results.keys())
    n_metrics = len(metric_names)

    # Colors for different metrics
    colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Get layers from first metric (all should have same layers)
    layers = sorted(all_results[metric_names[0]].keys())

    # R² plot - all metrics on same axes
    ax1 = axes[0]
    for i, metric_name in enumerate(metric_names):
        results = all_results[metric_name]
        test_r2_mean = [results[l]["test_r2_mean"] for l in layers]
        test_r2_std = [results[l]["test_r2_std"] for l in layers]

        color = colors[i % len(colors)]
        ax1.plot(layers, test_r2_mean, 'o-', label=metric_name, color=color, markersize=4)
        ax1.fill_between(
            layers,
            np.array(test_r2_mean) - np.array(test_r2_std),
            np.array(test_r2_mean) + np.array(test_r2_std),
            alpha=0.2, color=color
        )

    ax1.set_xlabel('Layer Index')
    ax1.set_ylabel('R² Score')
    ax1.set_title('Uncertainty Metric Predictability by Layer')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Best R² comparison bar chart
    ax2 = axes[1]
    best_r2s = []
    best_layers = []
    for metric_name in metric_names:
        results = all_results[metric_name]
        best_layer = max(results.keys(), key=lambda l: results[l]["test_r2_mean"])
        best_r2s.append(results[best_layer]["test_r2_mean"])
        best_layers.append(best_layer)

    bars = ax2.bar(metric_names, best_r2s, color=[colors[i % len(colors)] for i in range(n_metrics)])
    ax2.set_xlabel('Metric')
    ax2.set_ylabel('Best R² Score')
    ax2.set_title('Best R² by Metric')
    ax2.grid(True, alpha=0.3, axis='y')

    # Add layer labels on bars
    for bar, layer in zip(bars, best_layers):
        height = bar.get_height()
        ax2.annotate(f'L{layer}',
                     xy=(bar.get_x() + bar.get_width() / 2, height),
                     xytext=(0, 3),
                     textcoords="offset points",
                     ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nPlot saved to {output_path}")


def plot_entropy_distribution(entropies: np.ndarray, output_path: Path):
    """Plot entropy distribution histogram."""
    _, ax = plt.subplots(figsize=(8, 5))

    ax.hist(entropies, bins=30, edgecolor='black', alpha=0.7,
            weights=np.ones(len(entropies)) / len(entropies) * 100)
    ax.axvline(entropies.mean(), color='red', linestyle='--',
               label=f'Mean: {entropies.mean():.3f}')
    ax.axvline(np.median(entropies), color='orange', linestyle='--',
               label=f'Median: {np.median(entropies):.3f}')
    ax.set_xlabel('Entropy')
    ax.set_ylabel('Percentage')
    ax.set_title(f'Next-Token Entropy Distribution (n={len(entropies)})')
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Entropy distribution plot saved to {output_path}")


def load_activations(activations_path: Path) -> Tuple[Dict[int, np.ndarray], Dict[str, np.ndarray]]:
    """Load activations and metrics from saved file."""
    print(f"Loading activations from {activations_path}...")
    data = np.load(activations_path)

    activations = {
        int(k.split("_")[1]): data[k]
        for k in data.files if k.startswith("layer_")
    }

    # Load all available metrics
    metrics = {}
    for metric_name in AVAILABLE_METRICS:
        if metric_name in data.files:
            metrics[metric_name] = data[metric_name]
        elif metric_name == "entropy" and "entropies" in data.files:
            # Backward compatibility: old files have "entropies" key
            metrics["entropy"] = data["entropies"]

    if not metrics:
        raise ValueError(f"No metrics found in {activations_path}. Re-run without --plot-only.")

    print(f"Loaded {len(activations)} layers, {len(list(metrics.values())[0])} samples, {len(metrics)} metrics")
    return activations, metrics


def get_base_output_prefix() -> str:
    """Generate base output filename prefix (without metric) for shared files like activations."""
    model_short = get_model_short_name(BASE_MODEL_NAME)
    if MODEL_NAME != BASE_MODEL_NAME:
        adapter_short = get_model_short_name(MODEL_NAME)
        return str(OUTPUTS_DIR / f"{model_short}_adapter-{adapter_short}_nexttoken")
    return str(OUTPUTS_DIR / f"{model_short}_nexttoken")


def get_metric_output_prefix(metric: str) -> str:
    """Generate output filename prefix for metric-specific files."""
    return f"{get_base_output_prefix()}_{metric}"


def main():
    global METRIC

    parser = argparse.ArgumentParser(description="Train uncertainty probes on next-token prediction")
    parser.add_argument("--metric", type=str, default=METRIC, choices=AVAILABLE_METRICS,
                        help=f"Metric to probe (default: {METRIC})")
    parser.add_argument("--all-metrics", action="store_true",
                        help="Train probes for all metrics (takes ~5x longer than single metric)")
    parser.add_argument("--plot-only", action="store_true",
                        help="Skip extraction, load saved activations and retrain probes")
    args = parser.parse_args()

    METRIC = args.metric
    run_all_metrics = args.all_metrics
    print(f"Device: {DEVICE}")
    if run_all_metrics:
        print(f"Metrics: ALL ({', '.join(AVAILABLE_METRICS)})")
    else:
        print(f"Metric: {METRIC}")

    # Generate output prefix
    base_prefix = get_base_output_prefix()  # For shared files (activations)
    print(f"Base output prefix: {base_prefix}")

    # Define shared output paths (metric-agnostic)
    activations_path = Path(f"{base_prefix}_activations.npz")
    entropy_dist_path = Path(f"{base_prefix}_entropy_distribution.png")
    checkpoint_path = Path(f"{base_prefix}_checkpoint.npz")

    if args.plot_only:
        # Load existing activations
        if not activations_path.exists():
            raise FileNotFoundError(
                f"Activations file not found: {activations_path}. "
                "Run without --plot-only first."
            )
        activations, all_metrics = load_activations(activations_path)
    else:
        # Full run: load model and extract activations
        model, tokenizer, num_layers = load_model_and_tokenizer(
            BASE_MODEL_NAME,
            adapter_path=MODEL_NAME if MODEL_NAME != BASE_MODEL_NAME else None
        )

        # Find and load dataset
        dataset_path = find_dataset_path()
        dataset, dataset_config = load_dataset(dataset_path)

        # Extract activations and compute all uncertainty metrics (with checkpointing)
        activations, all_metrics = extract_all_activations_and_metrics(
            dataset, model, tokenizer, num_layers, checkpoint_path
        )

        # Save activations and all metrics (so we can reuse for different metrics)
        print("\nSaving activations and metrics...")
        np.savez_compressed(
            activations_path,
            **{f"layer_{i}": acts for i, acts in activations.items()},
            **all_metrics  # Save all metrics for reuse
        )
        print(f"Saved to {activations_path}")

    # Determine which metrics to train probes for
    if run_all_metrics:
        metrics_to_train = all_metrics  # All 5 metrics
    else:
        if METRIC not in all_metrics:
            raise ValueError(f"Metric '{METRIC}' not found in saved file. Available: {list(all_metrics.keys())}")
        metrics_to_train = {METRIC: all_metrics[METRIC]}  # Just the selected one

    # Train probes (bootstrap for confidence intervals) and extract directions
    results, directions = run_all_probes(activations, metrics_to_train)

    # Save results and directions for each metric
    for metric_name in metrics_to_train.keys():
        metric_target = metrics_to_train[metric_name]
        layer_results = results[metric_name]
        layer_directions = directions[metric_name]

        # Generate metric-specific output paths
        metric_prefix = get_metric_output_prefix(metric_name)
        metric_directions_path = Path(f"{metric_prefix}_directions.npz")
        metric_results_path = Path(f"{metric_prefix}_results.json")
        metric_plot_path = Path(f"{metric_prefix}_results.png")

        # Save directions
        directions_data = {
            f"layer_{layer_idx}": direction
            for layer_idx, direction in layer_directions.items()
        }
        directions_data["_metadata_model"] = np.array(BASE_MODEL_NAME)
        directions_data["_metadata_metric"] = np.array(metric_name)
        np.savez_compressed(metric_directions_path, **directions_data)
        print(f"Saved {metric_name} directions to {metric_directions_path}")

        # Save results with metadata
        output_data = {
            "config": {
                "base_model": BASE_MODEL_NAME,
                "adapter": MODEL_NAME if MODEL_NAME != BASE_MODEL_NAME else None,
                "metric": metric_name,
                "train_split": TRAIN_SPLIT,
                "probe_alpha": PROBE_ALPHA,
                "use_pca": USE_PCA,
                "pca_components": PCA_COMPONENTS,
                "n_bootstrap": N_BOOTSTRAP,
                "seed": SEED,
            },
            "metric_stats": {
                "mean": float(metric_target.mean()),
                "std": float(metric_target.std()),
                "min": float(metric_target.min()),
                "max": float(metric_target.max()),
                "variance": float(metric_target.var()),
                "median": float(np.median(metric_target)),
                "iqr": float(np.percentile(metric_target, 75) - np.percentile(metric_target, 25)),
            },
            "results": {
                str(k): v for k, v in layer_results.items()
            }
        }

        with open(metric_results_path, "w") as f:
            json.dump(output_data, f, indent=2)
        print(f"Saved {metric_name} results to {metric_results_path}")

        # Plot results for this metric
        plot_results({metric_name: layer_results}, metric_plot_path)

    # Print results summary (all metrics together)
    print_results(results)

    # Plot entropy distribution (using entropy metric if available)
    if "entropy" in all_metrics:
        plot_entropy_distribution(all_metrics["entropy"], entropy_dist_path)


if __name__ == "__main__":
    main()
